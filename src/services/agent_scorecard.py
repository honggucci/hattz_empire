"""
Hattz Empire - Agent Scorecard System
ì—ì´ì „íŠ¸/LLM ì„±ëŠ¥ ì¶”ì  ë° ë™ì  ë¼ìš°íŒ…ì„ ìœ„í•œ ì ìˆ˜ ì‹œìŠ¤í…œ

ëª©ì :
- ê° ì—ì´ì „íŠ¸/LLMì˜ ì„±ëŠ¥ ì¶”ì 
- ìë™ ê²€ì¦ (ì½”ë“œ ì‹¤í–‰, í…ŒìŠ¤íŠ¸ í†µê³¼)
- CEO í”¼ë“œë°± ìˆ˜ì§‘
- ì ìˆ˜ ê¸°ë°˜ ë™ì  ë¼ìš°íŒ… (ì„±ëŠ¥ ë‚®ìœ¼ë©´ ë‹¤ë¥¸ LLMìœ¼ë¡œ êµì²´)

ì €ì¥ì†Œ: MSSQL agent_logs í…Œì´ë¸”
"""
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional, Literal
from enum import Enum
import subprocess
import tempfile
import os

# DB í•¨ìˆ˜ import
try:
    from . import database as db
    HAS_DB = True
except ImportError:
    HAS_DB = False


class TaskResult(Enum):
    """ì‘ì—… ê²°ê³¼"""
    SUCCESS = "success"           # ì„±ê³µ
    PARTIAL = "partial"           # ë¶€ë¶„ ì„±ê³µ
    FAILURE = "failure"           # ì‹¤íŒ¨
    ERROR = "error"               # ì—ëŸ¬ ë°œìƒ
    PENDING = "pending"           # í‰ê°€ ëŒ€ê¸°ì¤‘
    REJECTED = "rejected"         # CEOê°€ ê±°ë¶€


class FeedbackType(Enum):
    """í”¼ë“œë°± ìœ í˜•"""
    AUTO_CODE_PASS = "auto_code_pass"      # ì½”ë“œ ìë™ ì‹¤í–‰ ì„±ê³µ
    AUTO_CODE_FAIL = "auto_code_fail"      # ì½”ë“œ ìë™ ì‹¤í–‰ ì‹¤íŒ¨
    AUTO_TEST_PASS = "auto_test_pass"      # í…ŒìŠ¤íŠ¸ ìë™ í†µê³¼
    AUTO_TEST_FAIL = "auto_test_fail"      # í…ŒìŠ¤íŠ¸ ìë™ ì‹¤íŒ¨
    CEO_APPROVE = "ceo_approve"            # CEO ìŠ¹ì¸ (ğŸ‘)
    CEO_REJECT = "ceo_reject"              # CEO ê±°ë¶€ (ğŸ‘)
    CEO_REDO = "ceo_redo"                  # CEO ì¬ì‘ì—… ìš”ì²­
    FOLLOW_UP_SUCCESS = "follow_up_ok"     # í›„ì† ì‘ì—… ì„±ê³µ
    FOLLOW_UP_FAIL = "follow_up_fail"      # í›„ì† ì‘ì—… ì‹¤íŒ¨


@dataclass
class AgentLog:
    """ì—ì´ì „íŠ¸ í™œë™ ë¡œê·¸"""
    id: str                                # ê³ ìœ  ID
    timestamp: datetime                    # ì‹œê°„
    session_id: str                        # ì„¸ì…˜ ID
    task_id: str                           # ì‘ì—… ID

    # ì—ì´ì „íŠ¸ ì •ë³´
    role: str                              # excavator, coder, qa, strategist, researcher
    engine: str                            # engine_1, engine_2, merged
    model: str                             # claude-opus-4-5, gpt-5.2, gemini-3-pro

    # ì‘ì—… ì •ë³´
    task_type: str                         # code, strategy, analysis, research
    task_summary: str                      # ì‘ì—… ìš”ì•½ (100ì)
    input_tokens: int = 0                  # ì…ë ¥ í† í°
    output_tokens: int = 0                 # ì¶œë ¥ í† í°
    latency_ms: int = 0                    # ì‘ë‹µ ì‹œê°„ (ms)
    cost_usd: float = 0.0                  # ë¹„ìš© ($)

    # ê²°ê³¼
    result: TaskResult = TaskResult.PENDING
    result_code: Optional[str] = None      # ì—ëŸ¬ ì½”ë“œ ë“±

    # í”¼ë“œë°±
    feedback: Optional[FeedbackType] = None
    feedback_timestamp: Optional[datetime] = None
    feedback_note: Optional[str] = None    # CEO ì½”ë©˜íŠ¸

    # ì ìˆ˜ (ê³„ì‚°ë¨)
    score_delta: int = 0                   # ì´ ì‘ì—…ìœ¼ë¡œ ì¸í•œ ì ìˆ˜ ë³€í™”

    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "timestamp": self.timestamp.isoformat(),
            "session_id": self.session_id,
            "task_id": self.task_id,
            "role": self.role,
            "engine": self.engine,
            "model": self.model,
            "task_type": self.task_type,
            "task_summary": self.task_summary,
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "latency_ms": self.latency_ms,
            "cost_usd": self.cost_usd,
            "result": self.result.value,
            "result_code": self.result_code,
            "feedback": self.feedback.value if self.feedback else None,
            "feedback_timestamp": self.feedback_timestamp.isoformat() if self.feedback_timestamp else None,
            "feedback_note": self.feedback_note,
            "score_delta": self.score_delta,
        }


@dataclass
class ModelScore:
    """ëª¨ë¸ë³„ ëˆ„ì  ì ìˆ˜"""
    model: str
    role: str

    # ì ìˆ˜
    total_score: int = 100                 # ì‹œì‘ ì ìˆ˜ 100

    # í†µê³„
    total_tasks: int = 0
    success_count: int = 0
    failure_count: int = 0
    error_count: int = 0

    # CEO í”¼ë“œë°±
    ceo_approve_count: int = 0
    ceo_reject_count: int = 0

    # ìë™ ê²€ì¦
    auto_pass_count: int = 0
    auto_fail_count: int = 0

    # ë¹„ìš©
    total_cost_usd: float = 0.0
    avg_latency_ms: float = 0.0

    @property
    def success_rate(self) -> float:
        if self.total_tasks == 0:
            return 0.0
        return self.success_count / self.total_tasks

    @property
    def ceo_approval_rate(self) -> float:
        total = self.ceo_approve_count + self.ceo_reject_count
        if total == 0:
            return 0.0
        return self.ceo_approve_count / total

    def to_dict(self) -> dict:
        return {
            "model": self.model,
            "role": self.role,
            "total_score": self.total_score,
            "total_tasks": self.total_tasks,
            "success_rate": f"{self.success_rate:.1%}",
            "ceo_approval_rate": f"{self.ceo_approval_rate:.1%}",
            "total_cost_usd": f"${self.total_cost_usd:.4f}",
            "avg_latency_ms": f"{self.avg_latency_ms:.0f}ms",
        }


# =============================================================================
# Score Calculation Rules
# =============================================================================

SCORE_RULES = {
    # ìë™ ê²€ì¦
    FeedbackType.AUTO_CODE_PASS: +10,
    FeedbackType.AUTO_CODE_FAIL: -15,
    FeedbackType.AUTO_TEST_PASS: +15,
    FeedbackType.AUTO_TEST_FAIL: -20,

    # CEO í”¼ë“œë°± (ê°€ì¥ ì¤‘ìš”)
    FeedbackType.CEO_APPROVE: +20,
    FeedbackType.CEO_REJECT: -25,
    FeedbackType.CEO_REDO: -10,

    # í›„ì† ì‘ì—…
    FeedbackType.FOLLOW_UP_SUCCESS: +15,
    FeedbackType.FOLLOW_UP_FAIL: -15,
}

# ëª¨ë¸ ê°€ê²© (per 1K tokens, 2026.01 ê¸°ì¤€)
MODEL_PRICING = {
    "claude-opus-4-5-20251101": {"input": 0.015, "output": 0.075},
    "claude-sonnet-4-20250514": {"input": 0.003, "output": 0.015},
    "gpt-5.2": {"input": 0.010, "output": 0.030},
    "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},  # ì €ë ´!
    "gemini-3-pro-preview": {"input": 0.00125, "output": 0.005},
}


# =============================================================================
# Agent Scorecard Manager (DB-based)
# =============================================================================

class AgentScorecard:
    """ì—ì´ì „íŠ¸ ì ìˆ˜ ê´€ë¦¬ì (MSSQL ê¸°ë°˜)"""

    def __init__(self):
        """ì´ˆê¸°í™” - DB í…Œì´ë¸” ìƒì„±"""
        self._initialized = False
        if HAS_DB:
            try:
                db.create_agent_logs_table()
                self._initialized = True
            except Exception as e:
                print(f"[Scorecard] DB init error: {e}")

    @property
    def logs(self) -> list:
        """ë¡œê·¸ ëª©ë¡ (DBì—ì„œ ì¡°íšŒ)"""
        if not HAS_DB or not self._initialized:
            return []
        try:
            return db.get_agent_logs(limit=100)
        except:
            return []

    def log_task(
        self,
        session_id: str,
        task_id: str,
        role: str,
        engine: str,
        model: str,
        task_type: str,
        task_summary: str,
        input_tokens: int = 0,
        output_tokens: int = 0,
        latency_ms: int = 0,
    ) -> str:
        """ìƒˆ ì‘ì—… ë¡œê·¸ ìƒì„± (DB ì €ì¥)"""
        log_id = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{role}_{engine}"

        # ë¹„ìš© ê³„ì‚°
        pricing = MODEL_PRICING.get(model, {"input": 0.01, "output": 0.03})
        cost = (input_tokens * pricing["input"] + output_tokens * pricing["output"]) / 1000

        if HAS_DB and self._initialized:
            try:
                db.add_agent_log(
                    log_id=log_id,
                    session_id=session_id,
                    task_id=task_id,
                    role=role,
                    engine=engine,
                    model=model,
                    task_type=task_type,
                    task_summary=task_summary[:200],
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    latency_ms=latency_ms,
                    cost_usd=cost,
                    result="pending"
                )
            except Exception as e:
                print(f"[Scorecard] DB log error: {e}")

        return log_id

    def add_feedback(
        self,
        log_id: str,
        feedback: FeedbackType,
        note: Optional[str] = None
    ) -> bool:
        """í”¼ë“œë°± ì¶”ê°€ (DB ì—…ë°ì´íŠ¸)"""
        score_delta = SCORE_RULES.get(feedback, 0)

        # ê²°ê³¼ ê²°ì •
        if feedback in [FeedbackType.CEO_APPROVE, FeedbackType.AUTO_CODE_PASS, FeedbackType.AUTO_TEST_PASS]:
            result = "success"
        elif feedback in [FeedbackType.CEO_REJECT, FeedbackType.AUTO_CODE_FAIL, FeedbackType.AUTO_TEST_FAIL]:
            result = "failure"
        elif feedback == FeedbackType.CEO_REDO:
            result = "partial"
        else:
            result = None

        if HAS_DB and self._initialized:
            try:
                return db.add_agent_feedback(
                    log_id=log_id,
                    feedback=feedback.value,
                    score_delta=score_delta,
                    note=note,
                    result=result
                )
            except Exception as e:
                print(f"[Scorecard] DB feedback error: {e}")
                return False
        return False

    def get_scores(self) -> dict:
        """ëª¨ë“  ì ìˆ˜ ì¡°íšŒ (DB ì§‘ê³„)"""
        if not HAS_DB or not self._initialized:
            return {}

        try:
            scores = db.get_model_scores()
            return {f"{s['model']}:{s['role']}": s for s in scores}
        except Exception as e:
            print(f"[Scorecard] DB scores error: {e}")
            return {}

    def get_best_model(self, role: str) -> Optional[str]:
        """ì—­í• ë³„ ìµœê³  ì ìˆ˜ ëª¨ë¸ ë°˜í™˜ (ë™ì  ë¼ìš°íŒ…ìš©)"""
        if not HAS_DB or not self._initialized:
            return None

        try:
            return db.get_best_model_for_role(role)
        except:
            return None

    def get_leaderboard(self) -> list[dict]:
        """ì „ì²´ ë¦¬ë”ë³´ë“œ (DB ì§‘ê³„)"""
        if not HAS_DB or not self._initialized:
            return []

        try:
            return db.get_model_scores()
        except:
            return []

    def get_role_summary(self, role: str) -> dict:
        """ì—­í• ë³„ ìš”ì•½"""
        scores = self.get_leaderboard()
        role_scores = [s for s in scores if s.get("role") == role]

        return {
            "role": role,
            "models": sorted(
                role_scores,
                key=lambda x: x.get("total_score", 0),
                reverse=True
            )
        }

    def get_recent_log_id(self, session_id: Optional[str] = None) -> Optional[str]:
        """ê°€ì¥ ìµœê·¼ ë¡œê·¸ ID ì¡°íšŒ"""
        if not HAS_DB or not self._initialized:
            return None

        try:
            return db.get_recent_log_id(session_id)
        except:
            return None


# =============================================================================
# Code Validator (ìë™ ê²€ì¦)
# =============================================================================

class CodeValidator:
    """ì½”ë“œ ìë™ ê²€ì¦ê¸°"""

    @staticmethod
    def validate_python(code: str, timeout: int = 10) -> tuple[bool, str]:
        """
        Python ì½”ë“œ ê²€ì¦ (syntax + dry-run)

        Returns:
            (success: bool, message: str)
        """
        # 1. Syntax Check
        try:
            compile(code, "<string>", "exec")
        except SyntaxError as e:
            return False, f"SyntaxError: {e}"

        # 2. Dry-run in sandbox
        try:
            with tempfile.NamedTemporaryFile(
                mode="w",
                suffix=".py",
                delete=False,
                encoding="utf-8"
            ) as f:
                # ìœ„í—˜í•œ import ì²´í¬
                dangerous = ["os.system", "subprocess", "eval(", "exec(", "__import__"]
                for d in dangerous:
                    if d in code:
                        return False, f"Dangerous code detected: {d}"

                f.write(code)
                temp_path = f.name

            # ì‹¤í–‰ (timeout ì ìš©)
            result = subprocess.run(
                ["python", temp_path],
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=tempfile.gettempdir()
            )

            # ì •ë¦¬
            os.unlink(temp_path)

            if result.returncode == 0:
                return True, "Code executed successfully"
            else:
                return False, f"RuntimeError: {result.stderr[:500]}"

        except subprocess.TimeoutExpired:
            return False, f"Timeout: Code took longer than {timeout}s"
        except Exception as e:
            return False, f"ValidationError: {str(e)}"

    @staticmethod
    def validate_syntax_only(code: str) -> tuple[bool, str]:
        """ë¬¸ë²•ë§Œ ì²´í¬ (ì‹¤í–‰ ì•ˆ í•¨)"""
        try:
            compile(code, "<string>", "exec")
            return True, "Syntax OK"
        except SyntaxError as e:
            return False, f"SyntaxError at line {e.lineno}: {e.msg}"


# =============================================================================
# Singleton
# =============================================================================

_scorecard: Optional[AgentScorecard] = None


def get_scorecard() -> AgentScorecard:
    """Scorecard ì‹±ê¸€í†¤"""
    global _scorecard
    if _scorecard is None:
        _scorecard = AgentScorecard()
    return _scorecard


def get_validator() -> CodeValidator:
    """Validator ì¸ìŠ¤í„´ìŠ¤"""
    return CodeValidator()


# =============================================================================
# CLI Test
# =============================================================================

def main():
    """í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("AGENT SCORECARD TEST")
    print("=" * 60)

    scorecard = AgentScorecard(log_dir="logs/test_scores")

    # í…ŒìŠ¤íŠ¸ ë¡œê·¸ ìƒì„±
    log1 = scorecard.log_task(
        session_id="test_001",
        task_id="task_001",
        role="coder",
        engine="engine_1",
        model="claude-opus-4-5-20251101",
        task_type="code",
        task_summary="RSI ê³„ì‚° í•¨ìˆ˜ êµ¬í˜„",
        input_tokens=500,
        output_tokens=1000,
        latency_ms=2500,
    )
    print(f"\n[LOG] Created: {log1.id}")

    # í”¼ë“œë°± ì¶”ê°€
    scorecard.add_feedback(log1.id, FeedbackType.AUTO_CODE_PASS)
    scorecard.add_feedback(log1.id, FeedbackType.CEO_APPROVE, "ì˜í–ˆì–´!")

    # ì ìˆ˜ í™•ì¸
    print("\n[LEADERBOARD]")
    for entry in scorecard.get_leaderboard():
        print(f"  {entry['model']}:{entry['role']} = {entry['total_score']} pts")

    # ì½”ë“œ ê²€ì¦ í…ŒìŠ¤íŠ¸
    print("\n[CODE VALIDATOR]")
    validator = CodeValidator()

    good_code = "print('Hello, World!')"
    bad_code = "print('Hello"

    ok, msg = validator.validate_python(good_code)
    print(f"  Good code: {ok} - {msg}")

    ok, msg = validator.validate_syntax_only(bad_code)
    print(f"  Bad code: {ok} - {msg}")


if __name__ == "__main__":
    main()
