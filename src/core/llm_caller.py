"""
Hattz Empire - LLM Caller
LLM API í˜¸ì¶œ ë° ì—ì´ì „íŠ¸ ë¡œì§

2026.01.04 ì—…ë°ì´íŠ¸:
- ë“€ì–¼ ì—”ì§„ ì™€ì´ì–´ë§ (Writer + Auditor íŒ¨í„´)
- ìœ„ì›íšŒ ìë™ ì†Œì§‘ + ëª¨ë¸ í• ë‹¹
- ë£¨í”„ ë¸Œë ˆì´ì»¤ ì¶”ê°€

2026.01.07 ì—…ë°ì´íŠ¸:
- Analyst íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì£¼ì… (GeminiëŠ” íŒŒì¼ì‹œìŠ¤í…œ ì ‘ê·¼ ë¶ˆê°€)
"""
import os
import time as time_module
import asyncio
import glob as glob_module
from typing import Optional, Tuple, Dict, Any

import sys
from pathlib import Path

# ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
root_dir = Path(__file__).parent.parent.parent
if str(root_dir) not in sys.path:
    sys.path.insert(0, str(root_dir))

from config import (
    MODELS, DUAL_ENGINES, SINGLE_ENGINES,
    get_system_prompt, ModelConfig,
    ENFORCE_OUTPUT_CONTRACT, CONTRACT_EXEMPT_AGENTS  # v2.5
)

# v2.6: Server Logger ì—°ë™
from src.utils.server_logger import log_llm_call, log_error, logger

# v2.6.1: Flow Monitor ì—°ë™ (ë¶€íŠ¸ë¡œë” ì›ì¹™ ì¤€ìˆ˜ ëª¨ë‹ˆí„°ë§)
from src.services.flow_monitor import get_flow_monitor

# v2.5: Output Contract + Format Gate
from src.core.contracts import (
    validate_output,
    get_contract,
    get_schema_prompt,
    extract_json_from_output,
    FormatGateError,
    CONTRACT_REGISTRY
)


# =============================================================================
# Analyst íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (GeminiëŠ” íŒŒì¼ì‹œìŠ¤í…œ ì ‘ê·¼ ë¶ˆê°€)
# =============================================================================

# í”„ë¡œì íŠ¸ë³„ ë£¨íŠ¸ ê²½ë¡œ ë§¤í•‘
PROJECT_PATHS = {
    "hattz_empire": Path(__file__).parent.parent.parent,  # í˜„ì¬ í”„ë¡œì íŠ¸
    "wpcn": Path("C:/Users/hahonggu/Desktop/coin_master/projects/wpcn-backtester-cli-noflask"),
}


def collect_project_context(project_name: str, max_files: int = 50, max_chars: int = 30000) -> str:
    """
    í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡°ì™€ ì£¼ìš” íŒŒì¼ ë‚´ìš©ì„ ìˆ˜ì§‘í•˜ì—¬ Analystì—ê²Œ ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ìƒì„±

    Args:
        project_name: í”„ë¡œì íŠ¸ëª… (hattz_empire, wpcn ë“±)
        max_files: ìµœëŒ€ íŒŒì¼ ìˆ˜
        max_chars: ìµœëŒ€ ë¬¸ì ìˆ˜

    Returns:
        str: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    """
    project_root = PROJECT_PATHS.get(project_name)
    if not project_root or not project_root.exists():
        return f"[ERROR] í”„ë¡œì íŠ¸ '{project_name}' ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    context_parts = []
    context_parts.append(f"# í”„ë¡œì íŠ¸: {project_name}")
    context_parts.append(f"# ê²½ë¡œ: {project_root}")
    context_parts.append("")

    # 1. íŒŒì¼ êµ¬ì¡° ìˆ˜ì§‘
    context_parts.append("## íŒŒì¼ êµ¬ì¡°")
    py_files = list(project_root.glob("**/*.py"))
    md_files = list(project_root.glob("**/*.md"))

    # __pycache__, .git, node_modules ì œì™¸
    exclude_dirs = {'__pycache__', '.git', 'node_modules', '.venv', 'venv', '.claude'}
    py_files = [f for f in py_files if not any(d in str(f) for d in exclude_dirs)]
    md_files = [f for f in md_files if not any(d in str(f) for d in exclude_dirs)]

    context_parts.append(f"- Python íŒŒì¼: {len(py_files)}ê°œ")
    context_parts.append(f"- Markdown íŒŒì¼: {len(md_files)}ê°œ")
    context_parts.append("")

    # 2. ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ ëª©ë¡
    context_parts.append("## ë””ë ‰í† ë¦¬ êµ¬ì¡°")
    dirs = {}
    for f in py_files[:max_files]:
        rel_path = f.relative_to(project_root)
        parent = str(rel_path.parent)
        if parent not in dirs:
            dirs[parent] = []
        dirs[parent].append(rel_path.name)

    for dir_name, files in sorted(dirs.items()):
        context_parts.append(f"  {dir_name}/")
        for fname in sorted(files)[:10]:  # ë””ë ‰í† ë¦¬ë‹¹ ìµœëŒ€ 10ê°œ
            context_parts.append(f"    - {fname}")
        if len(files) > 10:
            context_parts.append(f"    ... ì™¸ {len(files) - 10}ê°œ")
    context_parts.append("")

    # 3. ì£¼ìš” íŒŒì¼ ë‚´ìš© (CLAUDE.md, README.md, config.py ë“±)
    context_parts.append("## ì£¼ìš” íŒŒì¼ ë‚´ìš©")
    important_files = [
        "CLAUDE.md", "README.md", "config.py", "app.py",
        "src/core/llm_caller.py", "src/api/chat.py"
    ]

    total_chars = len("\n".join(context_parts))
    for fname in important_files:
        if total_chars >= max_chars:
            context_parts.append(f"\n[TRUNCATED] ìµœëŒ€ {max_chars}ì ì´ˆê³¼ë¡œ ì¤‘ë‹¨")
            break

        fpath = project_root / fname
        if fpath.exists():
            try:
                content = fpath.read_text(encoding='utf-8')
                # íŒŒì¼ë‹¹ ìµœëŒ€ 5000ì
                if len(content) > 5000:
                    content = content[:5000] + "\n... (truncated)"
                context_parts.append(f"\n### {fname}")
                context_parts.append("```")
                context_parts.append(content)
                context_parts.append("```")
                total_chars += len(content) + 100
            except Exception as e:
                context_parts.append(f"\n### {fname}")
                context_parts.append(f"[ERROR] ì½ê¸° ì‹¤íŒ¨: {e}")

    # 4. í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜ (í’ˆì§ˆ ì§€í‘œ)
    test_files = [f for f in py_files if 'test' in f.name.lower()]
    context_parts.append(f"\n## í…ŒìŠ¤íŠ¸ íŒŒì¼: {len(test_files)}ê°œ")
    for tf in test_files[:5]:
        context_parts.append(f"  - {tf.relative_to(project_root)}")

    return "\n".join(context_parts)


# =============================================================================
# ë“€ì–¼ ì—”ì§„ + ìœ„ì›íšŒ ì„¤ì •
# =============================================================================

# ë“€ì–¼ ì—”ì§„ ì—­í•  ì •ì˜ (Writer + Auditor + Stamp)
# v2.4.3: GPT-5 mini ì œê±°, Stamp = Sonnet 4 í†µì¼
# - Opus: "ë§Œë“œëŠ” ì†" (coder)
# - Sonnet 4: "ê²€ì—´/ë„ì¥" (auditor, stamp)
# - GPT-5.2 Thinking: "ë‡Œ" (strategist/excavator writer)
# - Gemini Flash: "ìˆ˜ì§‘ê¸°" (researcher writer)
DUAL_ENGINE_ROLES = {
    "coder": {
        "writer": "claude_cli",           # CLI Opus - silent_implementer
        "auditor": "claude_cli",           # CLI Sonnet 4 - devils_advocate_reviewer
        "stamp": "claude_cli",             # CLI Sonnet 4 - strict_verdict_clerk
        "description": "ì½”ë“œ ì‘ì„± + ë¦¬ë·° + ë„ì¥",
        "writer_profile": "coder",         # Opus
        "auditor_profile": "reviewer",     # Sonnet 4
        "stamp_profile": "reviewer",       # Sonnet 4
    },
    "strategist": {
        "writer": "gpt_thinking",          # GPT-5.2 Thinking Extended - systems_architect (ë‡Œ)
        "auditor": "claude_cli",           # CLI Sonnet 4 - reality_check_cto
        "stamp": "claude_cli",             # CLI Sonnet 4 - strict_verdict_clerk
        "description": "ì „ëµ ìˆ˜ë¦½ (ë‡Œ) + ê²€ì¦ + ë„ì¥",
        "auditor_profile": "reviewer",     # Sonnet 4
        "stamp_profile": "reviewer",       # Sonnet 4
    },
    "qa": {
        "writer": "claude_cli",            # CLI Sonnet 4 - test_designer
        "auditor": "claude_cli",           # CLI Sonnet 4 - breaker_qa
        "stamp": "claude_cli",             # CLI Sonnet 4 - strict_verdict_clerk
        "description": "í…ŒìŠ¤íŠ¸ ìƒì„± + ê²€ì¦ + ë„ì¥",
        "writer_profile": "qa",            # Sonnet 4
        "auditor_profile": "reviewer",     # Sonnet 4
        "stamp_profile": "reviewer",       # Sonnet 4
    },
    "researcher": {
        "writer": "perplexity_sonar",      # Perplexity Sonar Pro - source_harvester (ê²€ìƒ‰ íŠ¹í™”)
        "auditor": "claude_cli",           # CLI Sonnet 4 - fact_sentinel
        "stamp": "claude_cli",             # CLI Sonnet 4 - strict_verdict_clerk
        "description": "ë¦¬ì„œì¹˜ (Perplexity) + íŒ©íŠ¸ì²´í¬ + ë„ì¥",
        "auditor_profile": "reviewer",     # Sonnet 4
        "stamp_profile": "reviewer",       # Sonnet 4
    },
    "excavator": {
        "writer": "gpt_thinking",          # GPT-5.2 Thinking Extended - requirements_interrogator (ë‡Œ)
        "auditor": "claude_cli",           # CLI Sonnet 4 - ambiguity_sniffer_reviewer
        "stamp": "claude_cli",             # CLI Sonnet 4 - strict_verdict_clerk
        "description": "CEO ì˜ë„ ë°œêµ´ (ë‡Œ) + ê²€ì¦ + ë„ì¥",
        "auditor_profile": "reviewer",     # Sonnet 4
        "stamp_profile": "reviewer",       # Sonnet 4
    },
}

# VIP_DUAL_ENGINE ì‚­ì œë¨ (v2.4.4 - CEO í”„ë¦¬í”½ìŠ¤ ê¸°ëŠ¥ ì œê±°)

# ìœ„ì›íšŒë³„ ëª¨ë¸ í• ë‹¹ - CLI ê¸°ë°˜ (Claude Code CLI ì‚¬ìš©)
# v2.4: PM ì „ìš© ë‹¨ì¼ ìœ„ì›íšŒ - 7ê°œ í˜ë¥´ì†Œë‚˜ ì „ì› ì°¸ì—¬
COUNCIL_MODEL_MAPPING = {
    "pm": {
        "personas": {
            "skeptic": "cli",           # ğŸ¤¨ íšŒì˜ë¡ ì - ê·¼ê±° ìš”êµ¬
            "perfectionist": "cli",     # ğŸ”¬ ì™„ë²½ì£¼ì˜ì - ë””í…Œì¼ ì§‘ì°©
            "pragmatist": "cli",        # ğŸ¯ í˜„ì‹¤ì£¼ì˜ì - ì‹¤í–‰ ì¤‘ì‹¬
            "pessimist": "cli",         # ğŸ˜° ë¹„ê´€ë¡ ì - ìµœì•… ê°€ì •
            "optimist": "cli",          # ğŸ˜Š ë‚™ê´€ë¡ ì - ê°€ëŠ¥ì„± ë°œê²¬
            "devils_advocate": "cli",   # ğŸ˜ˆ ì•…ë§ˆì˜ ë³€í˜¸ì¸ - ë°˜ëŒ€ ì˜ê²¬
            "security_hawk": "cli",     # ğŸ¦… ë³´ì•ˆ ê°ì‹œì - ì·¨ì•½ì  íƒì§€
        },
        "tiebreaker": "cli",
        "use_cli": True,  # CLI ì‚¬ìš©
    },
}

# ë£¨í”„ ë¸Œë ˆì´ì»¤ ì„¤ì •
LOOP_BREAKER_CONFIG = {
    "MAX_STAGE_RETRY": 2,      # ê°™ì€ ë‹¨ê³„ ìµœëŒ€ ì¬ì‹œë„
    "MAX_TOTAL_STEPS": 8,      # ì „ì²´ ìµœëŒ€ ë‹¨ê³„
    "SIMILARITY_THRESHOLD": 0.85,  # ë°˜ë³µ ì‘ë‹µ ê°ì§€ (85% ìœ ì‚¬ë„)
    "ESCALATE_TO_CEO": True,   # ë£¨í”„ ê°ì§€ì‹œ CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜
}

# Haiku ëª¨ë¸ ì¶”ê°€ (ì €ë ´í•œ ìœ„ì›íšŒìš©) - config.pyì— ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
# ì´ì œ config.pyì— claude_haikuê°€ ì •ì˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì´ ë¸”ë¡ì€ í´ë°±ìš©ìœ¼ë¡œë§Œ ìœ ì§€

from src.infra.stream import get_stream
from src.core.router import get_router, route_message
from src.services import database as db
from src.services import executor
from src.services import rag
from src.services.agent_scorecard import get_scorecard
from src.services import cost_tracker


# =============================================================================
# LLM API Clients
# =============================================================================

THINKING_EXTEND_PREFIX = """
## THINKING EXTEND MODE ACTIVATED
You are operating in deep reasoning mode. Before providing any answer:

1. ANALYZE: Break down the problem into components
2. QUESTION: Identify assumptions and edge cases
3. EVALUATE: Consider alternative interpretations
4. SYNTHESIZE: Combine insights into coherent response

Do NOT skip reasoning steps. Prioritize correctness over brevity.
Think step-by-step internally before outputting your final structured response.

---

"""


def call_anthropic(model_config: ModelConfig, messages: list, system_prompt: str) -> str:
    """Anthropic API â†’ CLI ë¦¬ë‹¤ì´ë ‰íŠ¸ (v2.4.3 - API ë¹„ìš© 0ì›)"""
    from src.services.cli_supervisor import call_claude_cli

    # model_idë¡œ í”„ë¡œí•„ ê²°ì • (opus=coder, sonnet=reviewer)
    profile = "coder" if "opus" in model_config.model_id.lower() else "reviewer"

    return call_claude_cli(messages, system_prompt, profile)


def call_openai(model_config: ModelConfig, messages: list, system_prompt: str) -> tuple[str, int, int]:
    """
    OpenAI API í˜¸ì¶œ

    GPT-5.2 Extended Thinking ì§€ì›:
    - reasoning_effort: "high" or "xhigh" â†’ ì‹¤ì œ reasoning í† í° ì‚¬ìš©
    - reasoning_effortê°€ noneì´ ì•„ë‹ˆë©´ temperature/top_p ì‚¬ìš© ë¶ˆê°€

    Returns:
        (response_text, input_tokens, output_tokens)
    """
    try:
        import openai
        client = openai.OpenAI(api_key=os.getenv(model_config.api_key_env))

        # í”„ë¡¬í”„íŠ¸ ì£¼ì… (thinking_modeì¼ ë•Œ ì¶”ê°€ ì§€ì¹¨)
        if getattr(model_config, 'thinking_mode', False):
            system_prompt = THINKING_EXTEND_PREFIX + system_prompt

        full_messages = [{"role": "system", "content": system_prompt}]
        full_messages.extend(messages)

        # GPT-5.2 ê³„ì—´: reasoning_effort ì§€ì› + temperature ì¶©ëŒ ë°©ì§€
        if model_config.model_id.startswith("gpt-5"):
            # reasoning_effort ê°€ì ¸ì˜¤ê¸° (AGENT_CONFIGì—ì„œ ì„¤ì •)
            reasoning_effort = getattr(model_config, 'reasoning_effort', None)

            # reasoning_effortê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ (high/xhigh)
            # â†’ temperature/top_p ì‚¬ìš© ë¶ˆê°€ (OpenAI ì œì•½)
            if reasoning_effort and reasoning_effort != "none":
                print(f"[OpenAI] GPT-5.2 Thinking Extended: reasoning_effort={reasoning_effort}")
                response = client.chat.completions.create(
                    model=model_config.model_id,
                    max_completion_tokens=model_config.max_tokens,
                    reasoning_effort=reasoning_effort,  # â† ì‹¤ì œ Extended Thinking í™œì„±í™”
                    messages=full_messages
                )
            else:
                # reasoning_effort ì—†ìœ¼ë©´ ê¸°ë³¸ í˜¸ì¶œ (temperature ì‚¬ìš© ì•ˆ í•¨)
                response = client.chat.completions.create(
                    model=model_config.model_id,
                    max_completion_tokens=model_config.max_tokens,
                    messages=full_messages
                )
        else:
            # GPT-4 ì´í•˜: ê¸°ì¡´ ë°©ì‹
            response = client.chat.completions.create(
                model=model_config.model_id,
                max_tokens=model_config.max_tokens,
                temperature=model_config.temperature,
                messages=full_messages
            )

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
        usage = response.usage
        input_tokens = usage.prompt_tokens if usage else 0
        output_tokens = usage.completion_tokens if usage else 0

        return response.choices[0].message.content, input_tokens, output_tokens
    except Exception as e:
        return f"[OpenAI Error] {str(e)}", 0, 0


def call_google(model_config: ModelConfig, messages: list, system_prompt: str) -> tuple[str, int, int]:
    """
    Google Gemini API í˜¸ì¶œ

    Returns:
        (response_text, input_tokens, output_tokens)
    """
    try:
        if "gemini-3" in model_config.model_id:
            from google import genai
            client = genai.Client(api_key=os.getenv(model_config.api_key_env))

            contents = []
            for msg in messages:
                role = "user" if msg["role"] == "user" else "model"
                contents.append({"role": role, "parts": [{"text": msg["content"]}]})

            response = client.models.generate_content(
                model=model_config.model_id,
                contents=contents,
                config={
                    "system_instruction": system_prompt,
                    "temperature": model_config.temperature,
                    "max_output_tokens": model_config.max_tokens,
                }
            )

            # Gemini 3 í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
            input_tokens = 0
            output_tokens = 0
            if hasattr(response, 'usage_metadata'):
                input_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0) or 0
                output_tokens = getattr(response.usage_metadata, 'candidates_token_count', 0) or 0

            return response.text, input_tokens, output_tokens
        else:
            import google.generativeai as genai
            genai.configure(api_key=os.getenv(model_config.api_key_env))

            model = genai.GenerativeModel(
                model_name=model_config.model_id,
                system_instruction=system_prompt
            )

            history = []
            for msg in messages[:-1]:
                role = "user" if msg["role"] == "user" else "model"
                history.append({"role": role, "parts": [msg["content"]]})

            chat = model.start_chat(history=history)
            response = chat.send_message(messages[-1]["content"])

            # Gemini 1.5/2.0 í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (ê·¼ì‚¬ì¹˜)
            input_tokens = 0
            output_tokens = 0
            if hasattr(response, 'usage_metadata'):
                input_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0) or 0
                output_tokens = getattr(response.usage_metadata, 'candidates_token_count', 0) or 0

            return response.text, input_tokens, output_tokens
    except Exception as e:
        return f"[Google Error] {str(e)}", 0, 0


def call_llm(
    model_config: ModelConfig,
    messages: list,
    system_prompt: str,
    session_id: str = None,
    agent_role: str = None
) -> str:
    """
    LLM í˜¸ì¶œ ë¼ìš°í„° + ë¹„ìš© ê¸°ë¡

    Args:
        model_config: ëª¨ë¸ ì„¤ì •
        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        session_id: ì„¸ì…˜ ID (ë¹„ìš© ê¸°ë¡ìš©)
        agent_role: ì—ì´ì „íŠ¸ ì—­í•  (ë¹„ìš© ê¸°ë¡ìš©)

    Returns:
        LLM ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    input_tokens = 0
    output_tokens = 0
    response_text = ""

    if model_config.provider == "anthropic":
        response_text = call_anthropic(model_config, messages, system_prompt)
        # CLI í˜¸ì¶œì€ í† í° ì¶”ì  ì•ˆ í•¨ (ë¬´ë£Œ)
    elif model_config.provider == "openai":
        response_text, input_tokens, output_tokens = call_openai(model_config, messages, system_prompt)
    elif model_config.provider == "google":
        response_text, input_tokens, output_tokens = call_google(model_config, messages, system_prompt)
    elif model_config.provider == "claude_cli":
        # Claude Code CLI provider (EXEC tier) - ë¬´ë£Œ
        from src.services.cli_supervisor import call_claude_cli
        response_text = call_claude_cli(messages, system_prompt, getattr(model_config, 'profile', 'coder'))
    else:
        return f"[Error] Unknown provider: {model_config.provider}"

    # ë¹„ìš© ê¸°ë¡ (í† í°ì´ ìˆê³  ì—ëŸ¬ê°€ ì•„ë‹Œ ê²½ìš°)
    if input_tokens > 0 or output_tokens > 0:
        if not response_text.startswith("[") or not "Error]" in response_text:
            try:
                cost_tracker.record_api_call(
                    session_id=session_id or "unknown",
                    agent_role=agent_role or "unknown",
                    model_id=model_config.model_id,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens
                )
                print(f"[CostTracker] Recorded: {model_config.model_id} ({input_tokens}in/{output_tokens}out)")
            except Exception as e:
                print(f"[CostTracker] Failed to record: {e}")

    return response_text


def call_dual_engine(role: str, messages: list, system_prompt: str) -> str:
    """ë“€ì–¼ ì—”ì§„ í˜¸ì¶œ ë° ë³‘í•© (ë ˆê±°ì‹œ - config.py DUAL_ENGINES ì‚¬ìš©)"""
    config = DUAL_ENGINES.get(role)
    if not config:
        return f"[Error] Unknown dual engine role: {role}"

    response_1 = call_llm(config.engine_1, messages, system_prompt)
    response_2 = call_llm(config.engine_2, messages, system_prompt)

    if config.merge_strategy == "primary_fallback":
        if "[Error]" not in response_1:
            merged = f"""## {config.engine_1.name} (Primary)
{response_1}

---
## {config.engine_2.name} (Review)
{response_2}"""
        else:
            merged = response_2
    elif config.merge_strategy == "parallel":
        merged = f"""## {config.engine_1.name}
{response_1}

---
## {config.engine_2.name}
{response_2}"""
    else:
        merged = f"""## {config.engine_1.name}
{response_1}

---
## {config.engine_2.name}
{response_2}

---
**ë“€ì–¼ ì—”ì§„ ë¶„ì„ ì™„ë£Œ. ë‘ ê²°ê³¼ë¥¼ ë¹„êµ ê²€í† í•˜ì„¸ìš”.**"""

    stream = get_stream()
    stream.log_dual_engine(role, messages[-1]["content"], response_1, response_2, merged)

    return merged


# =============================================================================
# ë“€ì–¼ ì—”ì§„ V2 (Writer + Auditor íŒ¨í„´)
# =============================================================================

# Auditor JSON ìŠ¤í‚¤ë§ˆ (ì¶œë ¥ ê°•ì œìš©)
AUDITOR_JSON_SCHEMA = {
    "type": "object",
    "properties": {
        "verdict": {"type": "string", "enum": ["APPROVE", "REVISE", "REJECT"]},
        "must_fix": {"type": "array", "items": {"type": "string"}},
        "nice_to_fix": {"type": "array", "items": {"type": "string"}},
        "rewrite_instructions": {"type": "string"},
        "requires_council": {"type": "boolean"},
        "confidence": {"type": "number", "minimum": 0, "maximum": 100}
    },
    "required": ["verdict", "must_fix", "confidence"]
}


def _extract_json_from_text(text: str) -> dict:
    """
    í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ ì¶”ì¶œ (v2.3.3)

    ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡, ìˆœìˆ˜ JSON ëª¨ë‘ ì§€ì›
    """
    import json
    import re

    # 1ì°¨: ```json ... ``` ë¸”ë¡ ì°¾ê¸°
    json_block = re.search(r'```(?:json)?\s*\n?({[\s\S]*?})\s*\n?```', text)
    if json_block:
        try:
            return json.loads(json_block.group(1))
        except json.JSONDecodeError:
            pass

    # 2ì°¨: ìˆœìˆ˜ JSON ê°ì²´ ì°¾ê¸° (ì²« '{' ~ ë§ˆì§€ë§‰ '}')
    start = text.find('{')
    end = text.rfind('}')
    if start != -1 and end != -1 and end > start:
        try:
            return json.loads(text[start:end + 1])
        except json.JSONDecodeError:
            pass

    # 3ì°¨: ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
    return {
        "verdict": "REVISE",
        "must_fix": ["JSON íŒŒì‹± ì‹¤íŒ¨ - ì›ë³¸ í…ìŠ¤íŠ¸ í™•ì¸ í•„ìš”"],
        "nice_to_fix": [],
        "rewrite_instructions": text[:500],
        "requires_council": False,
        "confidence": 0
    }


def dual_engine_write_audit_rewrite(
    role: str,
    messages: list,
    system_prompt: str,
    max_rewrite: int = 3,
    session_id: str = None
) -> Tuple[str, Dict[str, Any]]:
    """
    ë“€ì–¼ ì—”ì§„ V3: Write â†’ Audit â†’ Rewrite íŒ¨í„´ (v2.3.3)

    ê¸°ì¡´ V2ì˜ "ë¶™ì—¬ë„£ê¸°" ë°©ì‹ ëŒ€ì‹ :
    1. Writerê°€ ì´ˆì•ˆ ì‘ì„±
    2. Auditorê°€ JSONìœ¼ë¡œ verdict ë°˜í™˜
    3. REVISEë©´ Writerê°€ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‘ì„± (ìµœëŒ€ max_rewriteíšŒ)
    4. APPROVEë©´ ì´ˆì•ˆ ê·¸ëŒ€ë¡œ ë°˜í™˜
    5. REJECTë©´ Council ì†Œì§‘ íŠ¸ë¦¬ê±°

    Returns:
        (ìµœì¢… ì‘ë‹µ, ë©”íƒ€ë°ì´í„°)
    """
    if role not in DUAL_ENGINE_ROLES:
        from src.services.cli_supervisor import CLISupervisor
        cli = CLISupervisor()
        result = cli.call_cli(messages[-1]["content"], system_prompt, "coder")
        return (result.output if result.success else f"[Error] {result.error}"), {"dual": False}

    config = DUAL_ENGINE_ROLES[role]
    writer_key = config["writer"]
    auditor_key = config["auditor"]
    writer_profile = config.get("writer_profile", "coder")
    auditor_profile = config.get("auditor_profile", "reviewer")

    rewrite_count = 0
    audit_history = []
    format_validated = False

    # 1ë‹¨ê³„: Writer ì´ˆì•ˆ ì‘ì„± (v2.5 Format Gate ì ìš©)
    print(f"[Dual-V3] {role} Writer ({writer_key}) ì´ˆì•ˆ ì‘ì„± ì¤‘...")
    draft, writer_name, format_validated = _call_with_contract(
        writer_key, messages, system_prompt, writer_profile, role, session_id=session_id
    )

    if "[Error]" in draft or "[CLI Error]" in draft:
        return draft, {"dual": True, "error": "writer_failed", "version": "v3"}

    # v2.5: Format Gate ê²½ê³  í‘œì‹œ
    if not format_validated and "[FORMAT_WARNING]" in draft:
        print(f"[Dual-V3] Writer ì¶œë ¥ í˜•ì‹ ê²€ì¦ ì‹¤íŒ¨, Auditorì—ê²Œ ì „ë‹¬")
        draft = draft.replace("[FORMAT_WARNING] ", "")

    while rewrite_count < max_rewrite:
        # 2ë‹¨ê³„: Auditor ë¦¬ë·° (JSON ì¶œë ¥ ê°•ì œ) - v2.4.2 ê°•í™”ëœ í”„ë¡¬í”„íŠ¸
        auditor_prompt = f"""ë‹¹ì‹ ì€ {role} ì‘ì—…ì˜ Auditor(ê°ì‚¬ê´€)ì…ë‹ˆë‹¤.

## ì ˆëŒ€ ê·œì¹™ (ìœ„ë°˜ ì‹œ ì¦‰ì‹œ ë¬´íš¨)
1. **ìˆ˜ì • ê¸ˆì§€**: "ë‚´ê°€ ê³ ì³ì¤„ê²Œìš”" ì ˆëŒ€ ê¸ˆì§€. ì˜¤ì§ íŒì •ë§Œ.
2. **ì¸ìš© í•„ìˆ˜**: ëª¨ë“  ì§€ì ì€ íŒŒì¼ê²½ë¡œ/í•¨ìˆ˜ëª…/ë¼ì¸/ì—ëŸ¬ ì¬í˜„ ì»¤ë§¨ë“œë¡œ ì¦ê±° ì œì‹œ.
   - "ëŠë‚Œìƒ ë³„ë¡œ" ê°™ì€ ê°ìƒë¬¸ = ì¦‰ì‹œ REJECT ì²˜ë¦¬ë¨
3. **Lazy Approval**: must_fixëŠ” Severity HIGHë§Œ í—ˆìš©:
   - ë³´ì•ˆ ì·¨ì•½ì  (ì¸ì¦/ê¶Œí•œ ìš°íšŒ, injection)
   - ë°ì´í„° ì†ìƒ/ìœ ì‹¤ ê°€ëŠ¥ì„±
   - í¬ë˜ì‹œ/ë¬´í•œë£¨í”„
   - í•µì‹¬ ê²½ë¡œ í…ŒìŠ¤íŠ¸ ë¶€ì¬
   - ëª…ë°±í•œ ìš”êµ¬ì‚¬í•­ ë¶ˆì¼ì¹˜
4. **ìŠ¤íƒ€ì¼/ì·¨í–¥/ë³€ìˆ˜ëª…** = nice_to_fixë¡œë§Œ (ë°˜ë ¤ ì‚¬ìœ  ë¶ˆê°€)

=== WRITER ê²°ê³¼ë¬¼ ===
{draft}
======================

**ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ì½”ë“œë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSON):**

{{
  "verdict": "APPROVE | REVISE | REJECT",
  "must_fix": [
    {{
      "severity": "HIGH",
      "issue": "ë¬¸ì œ ì„¤ëª…",
      "evidence": "íŒŒì¼:ë¼ì¸ ë˜ëŠ” ì¬í˜„ ì»¤ë§¨ë“œ",
      "fix_hint": "ìˆ˜ì • ë°©í–¥ (ì½”ë“œ ì•„ë‹˜)"
    }}
  ],
  "nice_to_fix": ["ê¶Œì¥ì‚¬í•­ (ë°˜ë ¤ ì‚¬ìœ  ì•„ë‹˜)"],
  "tests_to_add": ["ì¶”ê°€í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ëª…"],
  "evidence": ["ê²€ì¦ì— ì‚¬ìš©í•œ íŒŒì¼/í•¨ìˆ˜/ë¼ì¸ ëª©ë¡"],
  "risk_level": "LOW | MEDIUM | HIGH | CRITICAL",
  "requires_council": false,
  "confidence": 85,
  "rewrite_instructions": "REVISEì¼ ë•Œë§Œ: Writerì—ê²Œ ì „ë‹¬í•  êµ¬ì²´ì  ì§€ì‹œ"
}}

## Verdict ê¸°ì¤€
- **APPROVE**: must_fix ì—†ìŒ, ìš”êµ¬ì‚¬í•­ ì¶©ì¡±
- **REVISE**: must_fix 1ê°œ ì´ìƒ (HIGH severityë§Œ)
- **REJECT**: ê·¼ë³¸ì  ì„¤ê³„ ê²°í•¨ ë˜ëŠ” ìš”êµ¬ì‚¬í•­ ì™„ì „ ë¶ˆì¼ì¹˜ â†’ Council í•„ìˆ˜
"""

        auditor_messages = messages.copy()
        auditor_messages.append({"role": "assistant", "content": draft})
        auditor_messages.append({"role": "user", "content": auditor_prompt})

        print(f"[Dual-V3] {role} Auditor ({auditor_key}) ë¦¬ë·° ì¤‘...")
        auditor_response, auditor_name = _call_model_or_cli(
            auditor_key, auditor_messages, system_prompt, auditor_profile, session_id, f"{role}_auditor"
        )

        # JSON íŒŒì‹±
        audit = _extract_json_from_text(auditor_response)
        audit_history.append(audit)

        verdict = audit.get("verdict", "REVISE")
        print(f"[Dual-V3] Auditor verdict: {verdict} (confidence: {audit.get('confidence', 'N/A')})")

        # APPROVE: ì´ˆì•ˆ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if verdict == "APPROVE":
            meta = {
                "dual": True,
                "version": "v3",
                "writer_model": writer_name,
                "auditor_model": auditor_name,
                "role": role,
                "verdict": "APPROVE",
                "rewrite_count": rewrite_count,
                "audit_history": audit_history,
                "requires_council": audit.get("requires_council", False),
                "format_validated": format_validated,  # v2.5
            }
            return draft, meta

        # REJECT: Council íŠ¸ë¦¬ê±°ì™€ í•¨ê»˜ ë°˜í™˜
        if verdict == "REJECT":
            meta = {
                "dual": True,
                "version": "v3",
                "writer_model": writer_name,
                "auditor_model": auditor_name,
                "role": role,
                "verdict": "REJECT",
                "rewrite_count": rewrite_count,
                "audit_history": audit_history,
                "requires_council": True,  # REJECTë©´ ë¬´ì¡°ê±´ Council
                "rejection_reason": audit.get("must_fix", []),
                "format_validated": format_validated,  # v2.5
            }
            # REJECT ì‹œì—ë„ draft ë°˜í™˜ (Councilì—ì„œ ê²€í† ìš©)
            return f"""âš ï¸ **AUDITOR REJECT**

{draft}

---
**Rejection Reasons:**
{chr(10).join(f'- {item}' for item in audit.get('must_fix', []))}
""", meta

        # REVISE: Writerì—ê²Œ í”¼ë“œë°± ì „ë‹¬í•˜ì—¬ ì¬ì‘ì„±
        rewrite_count += 1
        print(f"[Dual-V3] Rewrite #{rewrite_count}...")

        rewrite_prompt = f"""ì´ì „ ì´ˆì•ˆì— ëŒ€í•´ Auditorê°€ ë‹¤ìŒ ìˆ˜ì •ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤:

**ë°˜ë“œì‹œ ìˆ˜ì •í•  í•­ëª©:**
{chr(10).join(f'- {item}' for item in audit.get('must_fix', []))}

**Auditor ì§€ì‹œì‚¬í•­:**
{audit.get('rewrite_instructions', 'ìœ„ í•­ëª©ë“¤ì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.')}

---

**ì´ì „ ì´ˆì•ˆ:**
{draft}

---

ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ìˆ˜ì •ëœ ë²„ì „ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""

        rewrite_messages = messages.copy()
        rewrite_messages.append({"role": "user", "content": rewrite_prompt})

        # v2.5 Format Gate ì ìš©
        draft, writer_name, format_validated = _call_with_contract(
            writer_key, rewrite_messages, system_prompt, writer_profile, role
        )

        if "[Error]" in draft or "[CLI Error]" in draft:
            return draft, {"dual": True, "error": "rewrite_failed", "version": "v3"}

        # v2.5: Format Gate ê²½ê³  ì œê±°
        if "[FORMAT_WARNING]" in draft:
            draft = draft.replace("[FORMAT_WARNING] ", "")

    # max_rewrite ì†Œì§„ ì‹œ ë§ˆì§€ë§‰ draft ë°˜í™˜
    meta = {
        "dual": True,
        "version": "v3",
        "writer_model": writer_name,
        "auditor_model": auditor_name,
        "role": role,
        "verdict": "MAX_REWRITE_EXHAUSTED",
        "rewrite_count": rewrite_count,
        "audit_history": audit_history,
        "requires_council": True,  # max_rewrite ì†Œì§„ ì‹œ Council ê¶Œì¥
        "format_validated": format_validated,  # v2.5
    }
    return draft, meta


def _call_model_or_cli(
    model_key: str,
    messages: list,
    system_prompt: str,
    profile: str = "coder",
    session_id: str = None,
    agent_role: str = None
) -> Tuple[str, str]:
    """
    ëª¨ë¸ ë˜ëŠ” CLI í˜¸ì¶œ í—¬í¼ í•¨ìˆ˜

    Args:
        model_key: ëª¨ë¸ í‚¤ ("claude_cli" ë˜ëŠ” MODELS í‚¤)
        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        profile: CLI í”„ë¡œí•„ (coder/qa/reviewer)
        session_id: ì„¸ì…˜ ID (ë¹„ìš© ì¶”ì ìš©)
        agent_role: ì—ì´ì „íŠ¸ ì—­í•  (ë¹„ìš© ì¶”ì ìš©)

    Returns:
        (ì‘ë‹µ, ëª¨ë¸ëª…)
    """
    if model_key == "claude_cli":
        from src.services.cli_supervisor import CLISupervisor
        cli = CLISupervisor()
        # ë©”ì‹œì§€ì—ì„œ ë§ˆì§€ë§‰ user ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = messages[-1]["content"] if messages else ""
        result = cli.call_cli(
            prompt=user_message,
            system_prompt=system_prompt,
            profile=profile,
            task_context=f"Dual Engine: {profile}"
        )
        if result.success:
            return result.output, f"Claude CLI ({profile})"
        else:
            return f"[CLI Error] {result.error or result.abort_reason}", f"Claude CLI ({profile})"
    else:
        model = MODELS.get(model_key, MODELS.get("gpt_5_mini"))
        return call_llm(model, messages, system_prompt, session_id, agent_role), model.name


def _call_with_contract(
    model_key: str,
    messages: list,
    system_prompt: str,
    profile: str,
    agent_role: str,
    max_retry: int = 3,
    session_id: str = None
) -> Tuple[str, str, bool]:
    """
    v2.5 Format Gate: LLM í˜¸ì¶œ + Output Contract ê²€ì¦

    Args:
        model_key: ëª¨ë¸ í‚¤
        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        profile: CLI í”„ë¡œí•„
        agent_role: ì—ì´ì „íŠ¸ ì—­í•  (coder, qa, reviewer ë“±)
        max_retry: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        session_id: ì„¸ì…˜ ID (ë¹„ìš© ì¶”ì ìš©)

    Returns:
        (ì‘ë‹µ, ëª¨ë¸ëª…, ê²€ì¦ì„±ê³µì—¬ë¶€)
    """
    contract = get_contract(agent_role)

    # Contractê°€ ì—†ëŠ” ì—­í• ì€ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
    if not contract:
        response, model_name = _call_model_or_cli(model_key, messages, system_prompt, profile, session_id, agent_role)
        return response, model_name, True

    # Schema í”„ë¡¬í”„íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
    schema_prompt = get_schema_prompt(agent_role)
    enhanced_prompt = f"{system_prompt}\n\n{schema_prompt}"

    last_error = None

    for attempt in range(max_retry):
        response, model_name = _call_model_or_cli(model_key, messages, enhanced_prompt, profile, session_id, agent_role)

        # ì—ëŸ¬ ì‘ë‹µì€ ê²€ì¦ ìŠ¤í‚µ
        if "[Error]" in response or "[CLI Error]" in response:
            return response, model_name, False

        # Output Contract ê²€ì¦
        success, validated, error = validate_output(response, agent_role)

        if success:
            print(f"[FormatGate] {agent_role} ê²€ì¦ ì„±ê³µ (attempt {attempt + 1})")
            # Pydantic ëª¨ë¸ì„ JSON ë¬¸ìì—´ë¡œ ë°˜í™˜
            if hasattr(validated, 'model_dump_json'):
                return validated.model_dump_json(indent=2), model_name, True
            return response, model_name, True

        # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ë¡œ ì¬ì‹œë„
        last_error = error
        print(f"[FormatGate] {agent_role} ê²€ì¦ ì‹¤íŒ¨ ({attempt + 1}/{max_retry}): {error[:100]}")

        if attempt < max_retry - 1:
            # ì—ëŸ¬ í”¼ë“œë°±ì„ í¬í•¨í•œ ì¬ì‹œë„ ë©”ì‹œì§€
            retry_prompt = f"""ì´ì „ ì‘ë‹µì´ í˜•ì‹ ì˜¤ë¥˜ë¡œ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤.

**ì˜¤ë¥˜ ë‚´ìš©:**
{error}

**ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ì‘ë‹µí•´ì£¼ì„¸ìš”.**

{schema_prompt}"""
            messages = messages.copy()
            messages.append({"role": "assistant", "content": response})
            messages.append({"role": "user", "content": retry_prompt})

    # ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ - ì›ë³¸ ì‘ë‹µ ë°˜í™˜ + ê²½ê³ 
    print(f"[FormatGate] {agent_role} ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼, ì›ë³¸ ì‘ë‹µ ì‚¬ìš©")
    return f"[FORMAT_WARNING] {response}", model_name, False


def call_dual_engine_v2(
    role: str,
    messages: list,
    system_prompt: str
) -> Tuple[str, Dict[str, Any]]:
    """
    ë“€ì–¼ ì—”ì§„ V2: Writer + Auditor íŒ¨í„´
    v2.4: Claude CLI ì§€ì› ì¶”ê°€

    1ë‹¨ê³„: Writerê°€ ì´ˆì•ˆ ì‘ì„± (API ë˜ëŠ” CLI)
    2ë‹¨ê³„: Auditorê°€ ë¦¬ë·° ë° ìˆ˜ì • ì œì•ˆ (API ë˜ëŠ” CLI)
    3ë‹¨ê³„: ì˜ê²¬ ë¶ˆì¼ì¹˜ì‹œ ë³‘í•© ë˜ëŠ” ìœ„ì›íšŒ ì†Œì§‘

    Returns:
        (ìµœì¢… ì‘ë‹µ, ë©”íƒ€ë°ì´í„°)
    """
    if role not in DUAL_ENGINE_ROLES:
        # ë“€ì–¼ ì—”ì§„ ì—­í• ì´ ì•„ë‹ˆë©´ CLIë¡œ í´ë°±
        from src.services.cli_supervisor import CLISupervisor
        cli = CLISupervisor()
        result = cli.call_cli(messages[-1]["content"], system_prompt, "coder")
        return (result.output if result.success else f"[Error] {result.error}"), {"dual": False}

    config = DUAL_ENGINE_ROLES[role]
    writer_key = config["writer"]
    auditor_key = config["auditor"]
    writer_profile = config.get("writer_profile", "coder")
    auditor_profile = config.get("auditor_profile", "reviewer")

    # 1ë‹¨ê³„: Writer ì´ˆì•ˆ ì‘ì„±
    print(f"[Dual-V2] {role} Writer ({writer_key}) ì‘ì—… ì¤‘...")
    writer_response, writer_name = _call_model_or_cli(writer_key, messages, system_prompt, writer_profile)

    if "[Error]" in writer_response or "[CLI Error]" in writer_response:
        return writer_response, {"dual": True, "error": "writer_failed"}

    # 2ë‹¨ê³„: Auditor ë¦¬ë·° - v2.4.2 ê°•í™”ëœ í”„ë¡¬í”„íŠ¸
    auditor_prompt = f"""ë‹¹ì‹ ì€ {role} ì‘ì—…ì˜ Auditor(ê°ì‚¬ê´€)ì…ë‹ˆë‹¤.

## ì ˆëŒ€ ê·œì¹™ (ìœ„ë°˜ ì‹œ ì¦‰ì‹œ ë¬´íš¨)
1. **ìˆ˜ì • ê¸ˆì§€**: "ë‚´ê°€ ê³ ì³ì¤„ê²Œìš”" ì ˆëŒ€ ê¸ˆì§€. ì˜¤ì§ íŒì •ë§Œ.
2. **ì¸ìš© í•„ìˆ˜**: ëª¨ë“  ì§€ì ì€ íŒŒì¼ê²½ë¡œ/í•¨ìˆ˜ëª…/ë¼ì¸/ì—ëŸ¬ ì¬í˜„ ì»¤ë§¨ë“œë¡œ ì¦ê±° ì œì‹œ.
3. **Lazy Approval**: must_fixëŠ” Severity HIGHë§Œ í—ˆìš©:
   - ë³´ì•ˆ ì·¨ì•½ì , ë°ì´í„° ì†ìƒ, í¬ë˜ì‹œ, í…ŒìŠ¤íŠ¸ ë¶€ì¬, ìš”êµ¬ì‚¬í•­ ë¶ˆì¼ì¹˜
4. **ìŠ¤íƒ€ì¼/ì·¨í–¥** = nice_to_fixë¡œë§Œ (ë°˜ë ¤ ì‚¬ìœ  ë¶ˆê°€)

=== WRITER ê²°ê³¼ë¬¼ ===
{writer_response}
======================

**ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ (ì½”ë“œë¸”ë¡ ì—†ì´):**

{{
  "verdict": "APPROVE | REVISE | REJECT",
  "must_fix": [{{"severity": "HIGH", "issue": "ë¬¸ì œ", "evidence": "íŒŒì¼:ë¼ì¸", "fix_hint": "ë°©í–¥"}}],
  "nice_to_fix": ["ê¶Œì¥ì‚¬í•­"],
  "evidence": ["ê²€ì¦í•œ íŒŒì¼/í•¨ìˆ˜ ëª©ë¡"],
  "risk_level": "LOW | MEDIUM | HIGH | CRITICAL",
  "requires_council": false,
  "confidence": 85
}}
"""

    auditor_messages = messages.copy()
    auditor_messages.append({"role": "assistant", "content": writer_response})
    auditor_messages.append({"role": "user", "content": auditor_prompt})

    print(f"[Dual-V2] {role} Auditor ({auditor_key}) ë¦¬ë·° ì¤‘...")
    auditor_response, auditor_name = _call_model_or_cli(auditor_key, auditor_messages, system_prompt, auditor_profile)

    # ê²°ê³¼ ë³‘í•©
    merged_response = f"""## ğŸ“ Writer ({writer_name})
{writer_response}

---

## ğŸ” Auditor ({auditor_name})
{auditor_response}

---
âœ… **ë“€ì–¼ ì—”ì§„ ê²€í†  ì™„ë£Œ** ({config['description']})
"""

    # ë©”íƒ€ë°ì´í„°
    meta = {
        "dual": True,
        "writer_model": writer_name,
        "auditor_model": auditor_name,
        "role": role,
        "description": config["description"],
    }

    # ë¡œê·¸
    stream = get_stream()
    stream.log_dual_engine(role, messages[-1]["content"], writer_response, auditor_response, merged_response)

    return merged_response, meta


# call_vip_dual_engine ì‚­ì œë¨ (v2.4.4 - CEO í”„ë¦¬í”½ìŠ¤ ê¸°ëŠ¥ ì œê±°)


# =============================================================================
# ìœ„ì›íšŒ í˜¸ì¶œ (Council Integration)
# =============================================================================

async def call_council_llm(
    system_prompt: str,
    user_message: str,
    temperature: float,
    persona_id: str = None,
    council_type: str = None
) -> str:
    """
    ìœ„ì›íšŒ í˜ë¥´ì†Œë‚˜ìš© CLI í˜¸ì¶œ (v2.3.2: API â†’ CLI ì „í™˜)

    ëª¨ë“  ìœ„ì›íšŒ ë©¤ë²„ê°€ Claude Code CLIë¥¼ ì‚¬ìš©
    """
    from src.services.cli_supervisor import CLISupervisor

    print(f"[Council-CLI] {persona_id} â†’ Claude Code CLI")

    cli_supervisor = CLISupervisor()

    # CLI í˜¸ì¶œ (reviewer í”„ë¡œí•„ - ì½ê¸° ì „ìš©)
    result = cli_supervisor.call_cli(
        prompt=user_message,
        system_prompt=system_prompt,
        profile="reviewer",
        task_context=f"Council: {council_type}, Persona: {persona_id}"
    )

    if result.success:
        return result.output
    else:
        error_msg = result.error or result.abort_reason or "CLI í˜¸ì¶œ ì‹¤íŒ¨"
        print(f"[Council-CLI] Error: {error_msg}")
        return f"[CLI ERROR] {error_msg}"


def init_council_with_llm():
    """ìœ„ì›íšŒì— CLI Caller ì£¼ì… (v2.3.2: API â†’ CLI ì „í™˜)"""
    from src.infra.council import get_council, reset_council
    from src.services.cli_supervisor import CLISupervisor

    # í•­ìƒ ì‹±ê¸€í†¤ ë¦¬ì…‹ í›„ ìƒˆë¡œ ìƒì„± (llm_caller í™•ì‹¤íˆ ì„¤ì •)
    reset_council()
    council = get_council()

    cli_supervisor = CLISupervisor()

    async def council_cli_caller(
        system_prompt: str,
        user_message: str,
        temperature: float,
        persona_id: str = None,
        council_type: str = None
    ) -> str:
        """ìœ„ì›íšŒ CLI í˜¸ì¶œ (Claude Code CLI ì‚¬ìš©)"""
        print(f"[Council-CLI] {persona_id} â†’ Claude Code CLI")

        # ë™ê¸° CLI í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        def sync_cli_call():
            # CLI í”„ë¡œí•„ ê²°ì • (v2.4.2: ìœ„ì›íšŒëŠ” council í”„ë¡œí•„ ì‚¬ìš©)
            profile = "council"

            # CLI í˜¸ì¶œ
            result = cli_supervisor.call_cli(
                prompt=user_message,
                system_prompt=system_prompt,
                profile=profile,
                task_context=f"Council: {council_type}, Persona: {persona_id}"
            )

            if result.success:
                # v2.4.2: None ì²´í¬ ì¶”ê°€
                return result.output or "[CLI ERROR] ë¹ˆ ì¶œë ¥"
            else:
                # CLI ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
                error_msg = result.error or result.abort_reason or "CLI í˜¸ì¶œ ì‹¤íŒ¨"
                print(f"[Council-CLI] Error: {error_msg}")
                return f"[CLI ERROR] {error_msg}"

        return await asyncio.get_event_loop().run_in_executor(None, sync_cli_call)

    council.set_llm_caller(council_cli_caller)
    print("[Council] CLI Caller ì£¼ì… ì™„ë£Œ (Claude Code CLI ì‚¬ìš©)")
    return council


def should_convene_council(
    agent_role: str,
    response: str,
    context: Dict = None,
    dual_meta: Dict = None
) -> Optional[str]:
    """
    ìœ„ì›íšŒ ìë™ ì†Œì§‘ ì¡°ê±´ íŒë‹¨ (v2.3.3 - JSON ê¸°ë°˜)

    PMë§Œ ìœ„ì›íšŒ ì†Œì§‘ ê°€ëŠ¥. ë‹¤ë¥¸ ì—ì´ì „íŠ¸ëŠ” ìœ„ì›íšŒ ë¶ˆí•„ìš”.

    v2.3.3 ë³€ê²½:
    - ë¬¸ìì—´ íƒì§€ ëŒ€ì‹  dual_metaì˜ requires_council í•„ë“œ ìš°ì„  ì‚¬ìš©
    - ë¬¸ìì—´ íƒì§€ëŠ” í´ë°±ìœ¼ë¡œë§Œ ì‚¬ìš©

    Args:
        agent_role: ì—ì´ì „íŠ¸ ì—­í• 
        response: ì—ì´ì „íŠ¸ ì‘ë‹µ
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        dual_meta: ë“€ì–¼ ì—”ì§„ ë©”íƒ€ë°ì´í„° (requires_council í•„ë“œ í¬í•¨)

    Returns:
        "pm" ë˜ëŠ” None
    """
    # PMë§Œ ìœ„ì›íšŒ ì†Œì§‘ ê°€ëŠ¥
    if agent_role != "pm":
        return None

    context = context or {}
    dual_meta = dual_meta or {}

    # =========================================================================
    # 1ìˆœìœ„: dual_metaì˜ requires_council í•„ë“œ (JSON ê¸°ë°˜)
    # =========================================================================
    if dual_meta.get("requires_council") is True:
        print(f"[Council] JSON ê¸°ë°˜ íŠ¸ë¦¬ê±°: requires_council=True (verdict: {dual_meta.get('verdict', 'N/A')})")
        return "pm"

    # REJECT verdictë©´ ë¬´ì¡°ê±´ Council
    if dual_meta.get("verdict") == "REJECT":
        print("[Council] JSON ê¸°ë°˜ íŠ¸ë¦¬ê±°: verdict=REJECT")
        return "pm"

    # MAX_REWRITE_EXHAUSTEDë©´ Council ê¶Œì¥
    if dual_meta.get("verdict") == "MAX_REWRITE_EXHAUSTED":
        print("[Council] JSON ê¸°ë°˜ íŠ¸ë¦¬ê±°: MAX_REWRITE_EXHAUSTED")
        return "pm"

    # =========================================================================
    # 2ìˆœìœ„: audit_historyì—ì„œ requires_council ì²´í¬
    # =========================================================================
    audit_history = dual_meta.get("audit_history", [])
    for audit in audit_history:
        if audit.get("requires_council") is True:
            print(f"[Council] audit_history íŠ¸ë¦¬ê±°: requires_council=True")
            return "pm"

    # =========================================================================
    # 3ìˆœìœ„: ë¬¸ìì—´ íƒì§€ (í´ë°± - ë ˆê±°ì‹œ í˜¸í™˜)
    # =========================================================================
    # ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì • ê°ì§€ (ì „ëµ/ë°©í–¥/ê²°ì •)
    decision_keywords = ["ì „ëµ", "strategy", "ë°©í–¥", "decision", "ê²°ì •", "plan", "ì•„í‚¤í…ì²˜", "architecture"]
    if any(kw in response.lower() for kw in decision_keywords):
        if len(response) > 500:  # ê¸´ ì‘ë‹µì¼ ë•Œë§Œ
            print("[Council] ë¬¸ìì—´ íƒì§€ íŠ¸ë¦¬ê±°: decision keywords")
            return "pm"

    # ë¦¬ìŠ¤í¬ ê´€ë ¨ ê°ì§€
    risk_keywords = ["risk", "ë¦¬ìŠ¤í¬", "ìœ„í—˜", "ì£¼ì˜", "ê²½ê³ ", "warning", "critical"]
    if any(kw in response.lower() for kw in risk_keywords):
        # ë‹¨ìˆœ ì–¸ê¸‰ì´ ì•„ë‹Œ ì‹¤ì œ ê²½ê³ ì¸ì§€ í™•ì¸ (ë¬¸ë§¥ ì²´í¬)
        risk_patterns = ["âš ï¸", "âŒ", "ğŸš¨", "REJECT", "HOLD", "critical issue"]
        if any(p in response for p in risk_patterns):
            print("[Council] ë¬¸ìì—´ íƒì§€ íŠ¸ë¦¬ê±°: risk patterns")
            return "pm"

    return None


def _determine_trigger_source(dual_meta: Dict) -> str:
    """
    dual_metaì—ì„œ Council íŠ¸ë¦¬ê±° ì†ŒìŠ¤ ê²°ì • (v2.3.3)

    Returns:
        íŠ¸ë¦¬ê±° ì†ŒìŠ¤ ë¬¸ìì—´
    """
    if not dual_meta:
        return "manual"

    verdict = dual_meta.get("verdict", "")

    if verdict == "REJECT":
        return "json_verdict_reject"
    elif verdict == "MAX_REWRITE_EXHAUSTED":
        return "json_verdict_max_rewrite"
    elif dual_meta.get("requires_council") is True:
        return "json_requires_council"

    # audit_historyì—ì„œ requires_council í™•ì¸
    audit_history = dual_meta.get("audit_history", [])
    for audit in audit_history:
        if audit.get("requires_council") is True:
            return "json_requires_council"

    return "keyword_detection"


async def convene_council_async(
    council_type: str,
    content: str,
    context: str = "",
    trigger_source: str = "manual",
    original_verdict_json: Dict = None
) -> Dict:
    """
    ë¹„ë™ê¸° ìœ„ì›íšŒ ì†Œì§‘ (v2.3.3 - JSON ê¸°ë°˜ íŠ¸ë¦¬ê±° ì§€ì›)

    Args:
        council_type: ìœ„ì›íšŒ ìœ í˜•
        content: ê²€í†  ëŒ€ìƒ ë‚´ìš©
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        trigger_source: íŠ¸ë¦¬ê±° ì†ŒìŠ¤
            - "manual": ìˆ˜ë™ ì†Œì§‘
            - "json_requires_council": JSON requires_council=True
            - "json_verdict_reject": JSON verdict=REJECT
            - "json_verdict_max_rewrite": MAX_REWRITE_EXHAUSTED
        original_verdict_json: íŠ¸ë¦¬ê±°ëœ ì›ë³¸ JSON verdict

    Returns:
        íŒì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    from src.infra.council import get_council, Verdict

    council = get_council()

    # LLM Callerê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì´ˆê¸°í™”
    if council.llm_caller is None:
        init_council_with_llm()

    print(f"[Council] {council_type.upper()} ìœ„ì›íšŒ ì†Œì§‘ ì¤‘... (trigger: {trigger_source})")
    verdict = await council.convene(
        council_type,
        content,
        context,
        trigger_source=trigger_source,
        original_verdict_json=original_verdict_json
    )

    result = {
        "council_type": council_type,
        "verdict": verdict.verdict.value,
        "average_score": verdict.average_score,
        "score_std": verdict.score_std,
        "requires_ceo": verdict.requires_ceo,
        "summary": verdict.summary,
        "trigger_source": verdict.trigger_source,
        "judges": [
            {
                "persona": j.persona_name,
                "icon": j.icon,
                "score": j.score,
                "reasoning": j.reasoning,
            }
            for j in verdict.judges
        ]
    }

    print(f"[Council] íŒì •: {verdict.verdict.value} (í‰ê·  {verdict.average_score}/10)")
    return result


def convene_council_sync(
    council_type: str,
    content: str,
    context: str = "",
    trigger_source: str = "manual",
    original_verdict_json: Dict = None
) -> Dict:
    """ë™ê¸° ë²„ì „ ìœ„ì›íšŒ ì†Œì§‘"""
    return asyncio.run(convene_council_async(
        council_type, content, context, trigger_source, original_verdict_json
    ))


# =============================================================================
# ë£¨í”„ ë¸Œë ˆì´ì»¤ (Loop Breaker)
# =============================================================================

class LoopBreaker:
    """
    ì—ì´ì „íŠ¸ ë£¨í”„ ê°ì§€ ë° ì°¨ë‹¨

    - MAX_STAGE_RETRY: ê°™ì€ ë‹¨ê³„ ìµœëŒ€ ì¬ì‹œë„
    - MAX_TOTAL_STEPS: ì „ì²´ ìµœëŒ€ ë‹¨ê³„
    - ë°˜ë³µ ì‘ë‹µ ê°ì§€ (ìœ ì‚¬ë„ ê¸°ë°˜)
    - CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜
    """

    def __init__(self):
        self.step_count = 0
        self.stage_retries: Dict[str, int] = {}
        self.response_history: list = []
        self.is_broken = False
        self.break_reason = None

    def reset(self):
        """ë¸Œë ˆì´ì»¤ ì´ˆê¸°í™”"""
        self.step_count = 0
        self.stage_retries = {}
        self.response_history = []
        self.is_broken = False
        self.break_reason = None

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ Jaccard)"""
        if not text1 or not text2:
            return 0.0

        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1 & words2
        union = words1 | words2

        return len(intersection) / len(union)

    def check_and_update(self, stage: str, response: str) -> Tuple[bool, Optional[str]]:
        """
        ë£¨í”„ ì²´í¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸

        Args:
            stage: í˜„ì¬ ë‹¨ê³„ (ì˜ˆ: "coder", "qa", "strategist")
            response: ì—ì´ì „íŠ¸ ì‘ë‹µ

        Returns:
            (should_break, break_reason): ì¤‘ë‹¨í•´ì•¼ í•˜ë©´ Trueì™€ ì‚¬ìœ 
        """
        config = LOOP_BREAKER_CONFIG

        # 1. ì „ì²´ ë‹¨ê³„ ìˆ˜ ì²´í¬
        self.step_count += 1
        if self.step_count > config["MAX_TOTAL_STEPS"]:
            self.is_broken = True
            self.break_reason = f"MAX_TOTAL_STEPS ì´ˆê³¼ ({self.step_count}/{config['MAX_TOTAL_STEPS']})"
            return True, self.break_reason

        # 2. ê°™ì€ ë‹¨ê³„ ì¬ì‹œë„ ì²´í¬
        self.stage_retries[stage] = self.stage_retries.get(stage, 0) + 1
        if self.stage_retries[stage] > config["MAX_STAGE_RETRY"]:
            self.is_broken = True
            self.break_reason = f"MAX_STAGE_RETRY ì´ˆê³¼: {stage} ({self.stage_retries[stage]}íšŒ)"
            return True, self.break_reason

        # 3. ë°˜ë³µ ì‘ë‹µ ê°ì§€
        for prev_response in self.response_history[-3:]:  # ìµœê·¼ 3ê°œì™€ ë¹„êµ
            similarity = self._calculate_similarity(response, prev_response)
            if similarity > config["SIMILARITY_THRESHOLD"]:
                self.is_broken = True
                self.break_reason = f"ë°˜ë³µ ì‘ë‹µ ê°ì§€ (ìœ ì‚¬ë„: {similarity:.2%})"
                return True, self.break_reason

        # 4. ì‘ë‹µ íˆìŠ¤í† ë¦¬ ì €ì¥
        self.response_history.append(response[:1000])  # ì²˜ìŒ 1000ìë§Œ

        return False, None

    def get_escalation_message(self) -> str:
        """CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜ ë©”ì‹œì§€ ìƒì„±"""
        return f"""
âš ï¸ **ë£¨í”„ ë¸Œë ˆì´ì»¤ ë°œë™**

**ì‚¬ìœ **: {self.break_reason}
**ì§„í–‰ ë‹¨ê³„**: {self.step_count}íšŒ
**ë‹¨ê³„ë³„ ì¬ì‹œë„**: {dict(self.stage_retries)}

---

**ê¶Œì¥ ì¡°ì¹˜**:
1. í˜„ì¬ ì‘ì—…ì„ ìˆ˜ë™ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”
2. ìš”ì²­ì„ ë” ëª…í™•í•˜ê²Œ ì¬ì •ì˜í•˜ì„¸ìš”
3. ì‘ì—… ë²”ìœ„ë¥¼ ì¶•ì†Œí•˜ì„¸ìš”

**ìë™ ì¡°ì¹˜**: ë£¨í”„ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

    def should_escalate_to_ceo(self) -> bool:
        """CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€"""
        return self.is_broken and LOOP_BREAKER_CONFIG.get("ESCALATE_TO_CEO", True)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_loop_breaker: Optional[LoopBreaker] = None


def get_loop_breaker() -> LoopBreaker:
    """LoopBreaker ì‹±ê¸€í†¤"""
    global _loop_breaker
    if _loop_breaker is None:
        _loop_breaker = LoopBreaker()
    return _loop_breaker


def check_loop(stage: str, response: str) -> Tuple[bool, Optional[str]]:
    """ë£¨í”„ ì²´í¬ í—¬í¼ í•¨ìˆ˜"""
    return get_loop_breaker().check_and_update(stage, response)


# =============================================================================
# Agent Call
# =============================================================================

def extract_project_from_message(message: str) -> tuple[str, str]:
    """
    [PROJECT: xxx] íƒœê·¸ì—ì„œ í”„ë¡œì íŠ¸ëª… ì¶”ì¶œ

    Returns:
        (project_name, message_without_project_tag)

    ì˜ˆì‹œ:
        "[PROJECT: test]\nì•ˆë…•" â†’ ("test", "ì•ˆë…•")
        "ê·¸ëƒ¥ ë©”ì‹œì§€" â†’ (None, "ê·¸ëƒ¥ ë©”ì‹œì§€")
    """
    import re
    match = re.match(r'\[PROJECT:\s*([^\]]+)\]\s*\n?(.*)', message, re.DOTALL)
    if match:
        project = match.group(1).strip()
        remaining = match.group(2).strip()
        return project, remaining
    return None, message


def call_agent(
    message: str,
    agent_role: str,
    auto_execute: bool = True,
    use_translation: bool = True,
    use_router: bool = True,
    return_meta: bool = False,
    use_dual_engine: bool = True,   # ë“€ì–¼ ì—”ì§„ ì‚¬ìš© ì—¬ë¶€
    auto_council: bool = True,      # ìœ„ì›íšŒ ìë™ ì†Œì§‘ ì—¬ë¶€
    _internal_call: bool = False,   # v2.3.3: PM ë‚´ë¶€ í˜¸ì¶œ í”Œë˜ê·¸ (í•˜ìœ„ ì—ì´ì „íŠ¸ í˜¸ì¶œìš©)
) -> str | tuple[str, dict]:
    """
    ì‹¤ì œ LLM í˜¸ì¶œ + [EXEC] íƒœê·¸ ìë™ ì‹¤í–‰ + RAG ì»¨í…ìŠ¤íŠ¸ ì£¼ì… + ë²ˆì—­ + ìŠ¤ì½”ì–´ì¹´ë“œ ë¡œê¹…

    v2.3.3 ë³€ê²½:
    - CEOëŠ” PMë§Œ í˜¸ì¶œ ê°€ëŠ¥. í•˜ìœ„ ì—ì´ì „íŠ¸(coder/qa/strategist ë“±)ëŠ” PMì´ í˜¸ì¶œ.
    - _internal_call=Trueë©´ PMì´ í•˜ìœ„ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ë¯€ë¡œ í—ˆìš©.

    v2.4.4 ë³€ê²½:
    - CEO í”„ë¦¬í”½ìŠ¤ ê¸°ëŠ¥ ì œê±° (ìµœê³ /, ìƒê°/, ê²€ìƒ‰/)
    - PMì€ Opus 4.5ë¡œ ê³ ì • (SAFETY í‹°ì–´)

    Args:
        return_meta: Trueì´ë©´ (response, meta_dict) íŠœí”Œ ë°˜í™˜
        _internal_call: Trueë©´ PM ë‚´ë¶€ í˜¸ì¶œ (í•˜ìœ„ ì—ì´ì „íŠ¸ í—ˆìš©)

    Returns:
        str ë˜ëŠ” (str, dict): response ë˜ëŠ” (response, model_meta)
    """
    from src.core.session_state import get_current_session

    # =========================================================================
    # v2.3.3: CEO â†’ PMë§Œ í—ˆìš©. í•˜ìœ„ ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ ì°¨ë‹¨.
    # =========================================================================
    ALLOWED_CEO_AGENTS = ["pm"]  # CEOê°€ ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸
    SUB_AGENTS = ["coder", "qa", "strategist", "analyst", "researcher", "excavator"]

    if not _internal_call and agent_role in SUB_AGENTS:
        print(f"[BLOCKED] CEO â†’ {agent_role} ì§ì ‘ í˜¸ì¶œ ì°¨ë‹¨. PMì„ í†µí•´ í˜¸ì¶œí•˜ì„¸ìš”.")
        error_msg = f"""âŒ **ì§ì ‘ í˜¸ì¶œ ì°¨ë‹¨ë¨**

CEOëŠ” í•˜ìœ„ ì—ì´ì „íŠ¸(`{agent_role}`)ë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì˜¬ë°”ë¥¸ íë¦„:**
1. CEO â†’ PMì—ê²Œ ìš”ì²­
2. PMì´ TaskSpec ìƒì„± â†’ í•˜ìœ„ ì—ì´ì „íŠ¸ì— ìœ„ì„

**ì˜ˆì‹œ:**
- âŒ "ì½”ë”ì•¼ ë²„ê·¸ ìˆ˜ì •í•´" (ì§ì ‘ í˜¸ì¶œ)
- âœ… "ë²„ê·¸ ìˆ˜ì •í•´ì¤˜" (PMì´ coderì—ê²Œ ìœ„ì„)
"""
        if return_meta:
            return error_msg, {"blocked": True, "reason": "direct_subagent_call"}
        return error_msg

    current_session_id = get_current_session()
    start_time = time_module.time()

    # ë””ë²„ê·¸: ì…ë ¥ ë©”ì‹œì§€ í™•ì¸
    import sys
    sys.stderr.write(f"[DEBUG-INPUT] message[:50]={message[:50] if len(message) > 50 else message}\n")
    sys.stderr.write(f"[DEBUG-INPUT] agent_role={agent_role}, _internal_call={_internal_call}\n")
    sys.stderr.flush()

    # [PROJECT: xxx] íƒœê·¸ì—ì„œ í”„ë¡œì íŠ¸ ì¶”ì¶œ
    current_project, message_without_project = extract_project_from_message(message)
    if current_project:
        print(f"[Project] Detected: {current_project}")

    router = get_router()
    routing = route_message(message, agent_role)

    # ëª¨ë¸ ë©”íƒ€ ì •ë³´ ìˆ˜ì§‘
    model_meta = {
        'model_name': routing.model_spec.name,
        'model_id': routing.model_spec.model_id,
        'tier': routing.model_tier,
        'reason': routing.reason,
        'provider': routing.model_spec.provider,
    }

    print(f"[Router] {agent_role} â†’ {routing.model_tier.upper()} ({routing.model_spec.name})")
    print(f"[Router] Reason: {routing.reason}")

    system_prompt = get_system_prompt(agent_role)
    if not system_prompt:
        return f"[Error] Unknown agent role: {agent_role}"

    # ë©”ì‹œì§€ ì²˜ë¦¬
    agent_message = message
    if use_translation and rag.is_korean(message):
        agent_message = rag.translate_for_agent(message)
        print(f"[Translate] CEOâ†’Agent: {len(message)}ì â†’ {len(agent_message)}ì")

    # =========================================================================
    # v2.5: ì—ì´ì „íŠ¸ë³„ RAG ì»¨í…ìŠ¤íŠ¸ ì£¼ì… (agent_filter í™œìš©)
    # - PM: ì „ì²´ ê²€ìƒ‰ (agent_filter=None)
    # - Coder/QA/Strategist: ì—ì´ì „íŠ¸ë³„ í•„í„°ë§ (ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë§Œ)
    # =========================================================================
    RAG_ENABLED_AGENTS = ["pm", "coder", "qa", "strategist", "researcher"]

    if agent_role in RAG_ENABLED_AGENTS:
        try:
            # PMì€ ì „ì²´ ê²€ìƒ‰, ë‚˜ë¨¸ì§€ëŠ” ì—ì´ì „íŠ¸ë³„ í•„í„°
            agent_filter = None if agent_role == "pm" else agent_role
            top_k = 5 if agent_role == "pm" else 3  # PMì€ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸

            rag_context = rag.build_context(
                agent_message,
                project=current_project,
                agent_filter=agent_filter,
                top_k=top_k,
                use_gemini=True,
                language="en",
                session_id=current_session_id
            )
            if rag_context:
                system_prompt = system_prompt + "\n\n" + rag_context
                filter_info = f"agent={agent_filter}" if agent_filter else "all"
                print(f"[RAG] Context injected ({current_project or 'all'}, {filter_info}): {len(rag_context)} chars")
        except Exception as e:
            print(f"[RAG] Context injection failed: {e}")

    # =========================================================================
    # v2.4.1: Analyst íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì£¼ì… (GeminiëŠ” íŒŒì¼ì‹œìŠ¤í…œ ì ‘ê·¼ ë¶ˆê°€)
    # =========================================================================
    if agent_role == "analyst" and current_project:
        try:
            project_context = collect_project_context(current_project)
            if project_context and not project_context.startswith("[ERROR]"):
                agent_message = f"""## í”„ë¡œì íŠ¸ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ (ìë™ ìˆ˜ì§‘)

{project_context}

---

## ë¶„ì„ ìš”ì²­

{agent_message}"""
                print(f"[Analyst] í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì£¼ì…: {len(project_context)} chars")
            else:
                print(f"[Analyst] í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {project_context}")
        except Exception as e:
            print(f"[Analyst] ì»¨í…ìŠ¤íŠ¸ ì£¼ì… ì‹¤íŒ¨: {e}")

    messages = []
    if current_session_id:
        db_messages = db.get_messages(current_session_id)
        for msg in db_messages:
            if msg.get('agent') == agent_role:
                messages.append({
                    "role": msg['role'],
                    "content": msg['content']
                })

    messages.append({"role": "user", "content": agent_message})

    # ë“€ì–¼ ì—”ì§„ ë©”íƒ€ë°ì´í„°
    dual_meta = {"dual": False}
    council_result = None

    # =========================================================================
    # ë“€ì–¼ ì—”ì§„ V3 ì‚¬ìš© (Write â†’ Audit â†’ Rewrite íŒ¨í„´)
    # =========================================================================
    if use_dual_engine and agent_role in DUAL_ENGINE_ROLES:
        print(f"[Dual-V3] {agent_role} Write-Audit-Rewrite íŒ¨í„´ í™œì„±í™”")
        response, dual_meta = dual_engine_write_audit_rewrite(agent_role, messages, system_prompt, session_id=current_session_id)

        # ìœ„ì›íšŒ ìë™ ì†Œì§‘ ì²´í¬ (dual_meta ì „ë‹¬) + FAIL ì‹œ ì¬ìˆ˜ì • ë£¨í”„
        MAX_COUNCIL_RETRY = 2  # ìœ„ì›íšŒ ì¬ìˆ˜ì • ìµœëŒ€ íšŸìˆ˜
        council_retry = 0

        if auto_council:
            council_type = should_convene_council(agent_role, response, dual_meta=dual_meta)

            while council_type and council_retry < MAX_COUNCIL_RETRY:
                # v2.3.3: trigger_source ê²°ì •
                trigger_source = _determine_trigger_source(dual_meta)
                print(f"[Council] ìë™ ì†Œì§‘ íŠ¸ë¦¬ê±°: {council_type} (source: {trigger_source}, retry: {council_retry})")
                try:
                    council_result = convene_council_sync(
                        council_type, response, agent_message,
                        trigger_source=trigger_source,
                        original_verdict_json=dual_meta.get("audit_history", [{}])[-1] if dual_meta.get("audit_history") else None
                    )
                    model_meta['council'] = council_result

                    # v2.4: FAILì´ë©´ ì¬ìˆ˜ì • ìš”ì²­
                    if council_result['verdict'] == 'fail' and council_retry < MAX_COUNCIL_RETRY - 1:
                        council_retry += 1
                        print(f"[Council] FAIL - ì¬ìˆ˜ì • ìš”ì²­ ({council_retry}/{MAX_COUNCIL_RETRY})")

                        # ìœ„ì›íšŒ í”¼ë“œë°±ìœ¼ë¡œ ì¬ìˆ˜ì • ìš”ì²­
                        concerns = [j.get('reasoning', '')[:200] for j in council_result['judges'] if j.get('score', 10) < 7]
                        feedback = "\n".join(concerns[:3]) if concerns else council_result['summary']

                        rewrite_prompt = f"""ìœ„ì›íšŒì—ì„œ ë‹¤ìŒ ë¬¸ì œë¥¼ ì§€ì í–ˆìŠµë‹ˆë‹¤:

{feedback}

ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì‘ë‹µì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”."""

                        rewrite_messages = messages.copy()
                        rewrite_messages.append({"role": "assistant", "content": response})
                        rewrite_messages.append({"role": "user", "content": rewrite_prompt})

                        # ì¬ìˆ˜ì • í˜¸ì¶œ
                        response, dual_meta = dual_engine_write_audit_rewrite(agent_role, rewrite_messages, system_prompt, session_id=current_session_id)
                        council_type = should_convene_council(agent_role, response, dual_meta=dual_meta)
                        continue

                    # PASS ë˜ëŠ” ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬ - ê²°ê³¼ ì¶”ê°€í•˜ê³  ì¢…ë£Œ
                    response += f"""

---

## ğŸ›ï¸ {council_type.upper()} ìœ„ì›íšŒ íŒì •

{council_result['summary']}

**ìƒì„¸ ì ìˆ˜:**
"""
                    for judge in council_result['judges']:
                        response += f"- {judge['icon']} {judge['persona']}: {judge['score']}/10 - {judge['reasoning'][:100]}...\n"
                    break

                except Exception as e:
                    print(f"[Council] ì†Œì§‘ ì‹¤íŒ¨: {e}")
                    break

        stream = get_stream()
        stream.log("ceo", agent_role, "request", agent_message)
        stream.log(agent_role, "ceo", "response", response)

    # =========================================================================
    # ë ˆê±°ì‹œ ë¼ìš°í„° ëª¨ë“œ (ë“€ì–¼ ì—”ì§„ ë¹„í™œì„±í™” ì‹œ)
    # =========================================================================
    elif use_router:
        response = router.call_model(routing, messages, system_prompt)
        print(f"[Router] Called: {routing.model_spec.name}")

        stream = get_stream()
        stream.log("ceo", agent_role, "request", agent_message)
        stream.log(agent_role, "ceo", "response", response)

        # ë¼ìš°í„° ëª¨ë“œì—ì„œë„ ìœ„ì›íšŒ ìë™ ì†Œì§‘ ì²´í¬
        if auto_council:
            council_type = should_convene_council(agent_role, response)
            if council_type:
                print(f"[Council] ìë™ ì†Œì§‘ íŠ¸ë¦¬ê±°: {council_type}")
                try:
                    council_result = convene_council_sync(council_type, response, agent_message)
                    model_meta['council'] = council_result

                    response += f"""

---

## ğŸ›ï¸ {council_type.upper()} ìœ„ì›íšŒ íŒì •

{council_result['summary']}
"""
                except Exception as e:
                    print(f"[Council] ì†Œì§‘ ì‹¤íŒ¨: {e}")

    # =========================================================================
    # ë ˆê±°ì‹œ ëª¨ë“œ (use_router=False)
    # v2.4: SINGLE_ENGINESì—ì„œ "claude_cli" ë¬¸ìì—´ ì§€ì›
    # =========================================================================
    else:
        if agent_role in DUAL_ENGINES:
            response = call_dual_engine(agent_role, messages, system_prompt)
        else:
            model_config = SINGLE_ENGINES.get(agent_role)
            if model_config:
                # v2.4: claude_cli ë¬¸ìì—´ì¸ ê²½ìš° CLI í˜¸ì¶œ
                if model_config == "claude_cli":
                    from config import CLI_PROFILES
                    profile = CLI_PROFILES.get(agent_role, "reviewer")
                    response, _ = _call_model_or_cli("claude_cli", messages, system_prompt, profile, current_session_id, agent_role)
                else:
                    response = call_llm(model_config, messages, system_prompt, current_session_id, agent_role)
                stream = get_stream()
                stream.log("ceo", agent_role, "request", agent_message)
                stream.log(agent_role, "ceo", "response", response)
            else:
                return f"[Error] No engine configured for: {agent_role}"

    # ë“€ì–¼ ì—”ì§„ ë©”íƒ€ ì •ë³´ ë³‘í•©
    model_meta['dual_engine'] = dual_meta

    if auto_execute and agent_role in ["coder", "pm"]:
        exec_results = executor.execute_all(response)
        if exec_results:
            exec_output = executor.format_results(exec_results)

            if agent_role == "pm":
                followup_prompt = f"""## EXEC ì‹¤í–‰ ê²°ê³¼

ë‹¤ìŒì€ ë°©ê¸ˆ ìš”ì²­í•œ ëª…ë ¹ì–´ë“¤ì˜ ì‹¤í–‰ ê²°ê³¼ì…ë‹ˆë‹¤:

{exec_output}

---

ìœ„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•´ì£¼ì„¸ìš”:
1. í•µì‹¬ ë°œê²¬ ì‚¬í•­ (ì´ëª¨ì§€ í¬í•¨)
2. ë‹¤ìŒ ì•¡ì…˜ ì œì•ˆ (ìˆë‹¤ë©´)
3. ì£¼ì˜ì ì´ë‚˜ ë¦¬ìŠ¤í¬ (ìˆë‹¤ë©´)

ê°„ê²°í•˜ê²Œ í•œê¸€ë¡œ ë³´ê³ í•´ì£¼ì„¸ìš”."""

                analysis_response = call_agent(
                    followup_prompt,
                    agent_role,
                    auto_execute=False,
                    use_translation=False
                )
                response += f"\n\n---\n\n## EXEC ê²°ê³¼ ë¶„ì„\n\n{analysis_response}"
            else:
                response += exec_output

    if use_translation and not rag.is_korean(response):
        response = rag.translate_for_ceo(response)
        print(f"[Translate] Agentâ†’CEO: í•œêµ­ì–´ë¡œ ë²ˆì—­ ì™„ë£Œ")

    try:
        elapsed_ms = int((time_module.time() - start_time) * 1000)
        scorecard = get_scorecard()

        if use_router:
            model_name = routing.model_spec.model_id
            engine_type = f"router_{routing.model_tier}"
        elif agent_role in DUAL_ENGINES:
            model_name = DUAL_ENGINES[agent_role].engine_1.model_id
            engine_type = "dual"
        elif agent_role in SINGLE_ENGINES:
            model_name = SINGLE_ENGINES[agent_role].model_id
            engine_type = "single"
        else:
            model_name = "unknown"
            engine_type = "unknown"

        task_type_map = {
            'excavator': 'analysis',
            'coder': 'code',
            'strategist': 'strategy',
            'qa': 'test',
            'analyst': 'analysis',
            'researcher': 'research',
            'pm': 'orchestration'
        }

        scorecard.log_task(
            session_id=current_session_id or "no_session",
            task_id=f"task_{int(time_module.time())}",
            role=agent_role,
            engine=engine_type,
            model=model_name,
            task_type=task_type_map.get(agent_role, 'general'),
            task_summary=message[:100],
            input_tokens=len(message.split()) * 2,
            output_tokens=len(response.split()) * 2,
            latency_ms=elapsed_ms
        )
        print(f"[Scorecard] Logged: {agent_role} â†’ {model_name} ({elapsed_ms}ms)")

        # ë©”íƒ€ ì •ë³´ì— ì¶”ê°€ ë°ì´í„° ì—…ë°ì´íŠ¸
        model_meta['latency_ms'] = elapsed_ms

        # v2.6: Server Loggerì— LLM í˜¸ì¶œ ê¸°ë¡
        log_llm_call(
            agent=agent_role,
            provider=model_meta.get('provider', 'unknown'),
            model=model_name,
            tokens=model_meta.get('input_tokens', 0) + model_meta.get('output_tokens', 0),
            cost=model_meta.get('cost', 0.0),
            duration_ms=elapsed_ms,
            success=True,
            session_id=current_session_id
        )
    except Exception as e:
        print(f"[Scorecard] Error: {e}")
        log_error(f"Scorecard logging failed: {e}", agent=agent_role, exc_info=False)

    # =========================================================================
    # v2.6.1: Flow Monitor - ë¶€íŠ¸ë¡œë” ì›ì¹™ ì¤€ìˆ˜ ëª¨ë‹ˆí„°ë§
    # - ì—­í•  ì¹¨ë²”, ì¡ë‹´, JSON ê³„ì•½ ê²€ì¦
    # =========================================================================
    flow_monitor = get_flow_monitor()
    flow_result = flow_monitor.validate_output(agent_role, response, current_session_id or "no_session")
    model_meta['flow_monitor'] = flow_result

    if flow_result['violations']:
        print(f"[FlowMonitor] WARN {agent_role} violation {len(flow_result['violations'])}ê±´: {flow_result['violations'][:2]}")
    else:
        print(f"[FlowMonitor] OK {agent_role} output validated")

    # =========================================================================
    # v2.5: Output Contract ê²€ì¦ (í˜•ì‹ ê²Œì´íŠ¸)
    # - CONTRACT_EXEMPT_AGENTS: Perplexity, Gemini ë“± JSON ê°•ì œ ë¶ˆê°€ ì—ì´ì „íŠ¸ ì œì™¸
    # - ENFORCE_OUTPUT_CONTRACT: Trueë©´ Fail Fast, Falseë©´ Soft Landing
    # - ABORT ë©”ì‹œì§€ëŠ” Contract ê²€ì¦ ê±´ë„ˆëœ€
    # =========================================================================
    is_abort_response = response.strip().startswith("# ABORT:")
    if agent_role in CONTRACT_REGISTRY and agent_role not in CONTRACT_EXEMPT_AGENTS and not is_abort_response:
        success, validated_or_raw, error_msg = validate_output(response, agent_role)
        if success:
            print(f"[FormatGate] OK {agent_role} output validated")
            model_meta['format_validated'] = True
            model_meta['validated_output'] = validated_or_raw.model_dump() if hasattr(validated_or_raw, 'model_dump') else None
        else:
            print(f"[FormatGate] FAIL {agent_role} format error: {error_msg[:100]}")
            model_meta['format_validated'] = False
            model_meta['format_error'] = error_msg

            # Fail Fast ëª¨ë“œ: í™˜ê²½ë³€ìˆ˜ ENFORCE_OUTPUT_CONTRACT=true ì‹œ ì˜ˆì™¸ ë°œìƒ
            if ENFORCE_OUTPUT_CONTRACT:
                raise FormatGateError(
                    f"[{agent_role}] Output Contract ìœ„ë°˜: {error_msg[:200]}"
                )
    elif is_abort_response:
        print(f"[FormatGate] SKIP {agent_role} - ABORT response")

    if return_meta:
        return response, model_meta
    return response


def process_call_tags(pm_response: str, use_loop_breaker: bool = True) -> list:
    """
    PM ì‘ë‹µì—ì„œ [CALL:agent] íƒœê·¸ë¥¼ ì²˜ë¦¬

    Args:
        pm_response: PM ì‘ë‹µ í…ìŠ¤íŠ¸
        use_loop_breaker: ë£¨í”„ ë¸Œë ˆì´ì»¤ ì‚¬ìš© ì—¬ë¶€

    Returns:
        ì—ì´ì „íŠ¸ í˜¸ì¶œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    calls = executor.extract_call_info(pm_response)
    results = []

    # ë£¨í”„ ë¸Œë ˆì´ì»¤ ì´ˆê¸°í™” (ìƒˆ íƒœìŠ¤í¬ ì‹œì‘)
    if use_loop_breaker:
        loop_breaker = get_loop_breaker()
        # ìƒˆ PM í˜¸ì¶œì´ë©´ ë¦¬ì…‹í•˜ì§€ ì•ŠìŒ (ì—°ì† ì‘ì—… ì¶”ì )

    for call in calls:
        agent = call['agent']
        message = call['message']

        # ë£¨í”„ ë¸Œë ˆì´ì»¤ ì²´í¬
        if use_loop_breaker:
            should_break, break_reason = check_loop(agent, message)
            if should_break:
                print(f"[LoopBreaker] STOP loop detected: {break_reason}")
                escalation_msg = get_loop_breaker().get_escalation_message()

                results.append({
                    'agent': 'loop_breaker',
                    'message': break_reason,
                    'response': escalation_msg,
                    'is_break': True
                })

                # CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜
                if get_loop_breaker().should_escalate_to_ceo():
                    print("[LoopBreaker] WARN CEO escalation required")

                break  # ë” ì´ìƒ ì—ì´ì „íŠ¸ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ

        print(f"[CALL] PM â†’ {agent}: {message[:100]}...")

        # PMì´ ì„œë¸Œì—ì´ì „íŠ¸ í˜¸ì¶œ ì‹œ _internal_call=True (CEO ì§ì ‘ í˜¸ì¶œ ì°¨ë‹¨ ìš°íšŒ)
        response = call_agent(message, agent, auto_execute=True, use_translation=False, _internal_call=True)

        # ì‘ë‹µ ê¸°ë°˜ ë£¨í”„ ì²´í¬
        if use_loop_breaker:
            should_break, break_reason = check_loop(f"{agent}_response", response)
            if should_break:
                print(f"[LoopBreaker] STOP repeated response: {break_reason}")
                response += f"\n\n---\n\n**[LoopBreaker Warning]**: {break_reason}"

        results.append({
            'agent': agent,
            'message': message,
            'response': response
        })

        print(f"[CALL] {agent} ì™„ë£Œ: {len(response)}ì")

    return results


def build_call_results_prompt(call_results: list) -> str:
    """í•˜ìœ„ ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ PMì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    prompt = "í•˜ìœ„ ì—ì´ì „íŠ¸ë“¤ì˜ ì‹¤í–‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•´ì£¼ì„¸ìš”.\n\n"

    for i, result in enumerate(call_results, 1):
        prompt += f"## {i}. {result['agent'].upper()} ì‘ë‹µ\n"
        prompt += f"**ìš”ì²­:** {result['message'][:200]}...\n\n"
        prompt += f"**ê²°ê³¼:**\n{result['response']}\n\n"
        prompt += "---\n\n"

    prompt += "ìœ„ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ CEOì—ê²Œ í•œê¸€ë¡œ ë³´ê³ í•´ì£¼ì„¸ìš”."
    return prompt


def mock_agent_response(message: str, agent_role: str) -> str:
    """Mock ì‘ë‹µ (í…ŒìŠ¤íŠ¸ìš©)"""
    responses = {
        'pm': f"""```yaml
sprint_plan:
  do:
    - "ìš”ì²­ ë¶„ì„: {message}"
    - "ì„¸ë¶€ íƒœìŠ¤í¬ ë¶„í•´"
    - "ì—ì´ì „íŠ¸ í• ë‹¹"
  dont:
    - "í¬ë§íšŒë¡œ ê¸ˆì§€"
    - "ëœ¬êµ¬ë¦„ ê³„íš ê¸ˆì§€"

delegation:
  - agent: "excavator"
    task: "CEO ì˜ë„ ë°œêµ´"
```

**Pragmatist + Skeptic ìŠ¤íƒ ìŠ¤** - êµ¬ì²´í™” í•„ìš”""",

        'coder': f"**CODER** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'excavator': f"**EXCAVATOR** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'strategist': f"**STRATEGIST** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'qa': f"**QA** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'analyst': f"**ANALYST** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'researcher': f"**RESEARCHER** Mock ì‘ë‹µ - ìš”ì²­: {message}",
    }

    return responses.get(agent_role, f"**{agent_role.upper()}** Mock ì‘ë‹µ - ìš”ì²­: {message}")
