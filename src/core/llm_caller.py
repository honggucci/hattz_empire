"""
Hattz Empire - LLM Caller
LLM API í˜¸ì¶œ ë° ì—ì´ì „íŠ¸ ë¡œì§

2026.01.04 ì—…ë°ì´íŠ¸:
- ë“€ì–¼ ì—”ì§„ ì™€ì´ì–´ë§ (Writer + Auditor íŒ¨í„´)
- ìœ„ì›íšŒ ìë™ ì†Œì§‘ + ëª¨ë¸ í• ë‹¹
- ë£¨í”„ ë¸Œë ˆì´ì»¤ ì¶”ê°€
"""
import os
import time as time_module
import asyncio
from typing import Optional, Tuple, Dict, Any

import sys
from pathlib import Path

# ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
root_dir = Path(__file__).parent.parent.parent
if str(root_dir) not in sys.path:
    sys.path.insert(0, str(root_dir))

from config import (
    MODELS, DUAL_ENGINES, SINGLE_ENGINES,
    get_system_prompt, ModelConfig
)


# =============================================================================
# ë“€ì–¼ ì—”ì§„ + ìœ„ì›íšŒ ì„¤ì •
# =============================================================================

# ë“€ì–¼ ì—”ì§„ ì—­í•  ì •ì˜ (Writer + Auditor)
DUAL_ENGINE_ROLES = {
    "coder": {
        "writer": "claude_sonnet",      # Sonnet 4 - ë¹ ë¥¸ ì½”ë“œ ì‘ì„±
        "auditor": "gpt_5_mini",         # GPT-5 mini - ì €ë ´í•œ ë¦¬ë·°
        "description": "ì½”ë“œ ì‘ì„± + ë¦¬ë·°"
    },
    "strategist": {
        "writer": "gpt_thinking",        # GPT-5.2 Thinking - ì „ëµ ìˆ˜ë¦½
        "auditor": "claude_sonnet",      # Sonnet - ì „ëµ ê²€ì¦
        "description": "ì „ëµ ìˆ˜ë¦½ + ê²€ì¦"
    },
    "qa": {
        "writer": "gpt_5_mini",          # GPT-5 mini - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ìƒì„±
        "auditor": "claude_sonnet",      # Sonnet - ë³´ì•ˆ/ì—£ì§€ì¼€ì´ìŠ¤ ê²€ì¦
        "description": "í…ŒìŠ¤íŠ¸ ìƒì„± + ê²€ì¦"
    },
    "researcher": {
        "writer": "gemini_flash",        # Gemini 3 Flash - ê²€ìƒ‰/ìˆ˜ì§‘
        "auditor": "gpt_5_mini",         # GPT-5 mini - íŒ©íŠ¸ì²´í¬
        "description": "ë¦¬ì„œì¹˜ + ê²€ì¦"
    },
    "excavator": {
        "writer": "claude_sonnet",       # Sonnet - ì˜ë„ íŒŒì•…
        "auditor": "gpt_5_mini",         # GPT-5 mini - í™•ì¸
        "description": "CEO ì˜ë„ ë°œêµ´ + í™•ì¸"
    },
}

# VIP í”„ë¦¬í”½ìŠ¤ìš© ë“€ì–¼ ì—”ì§„ (VIP Writer + VIP Auditor)
VIP_DUAL_ENGINE = {
    "ìµœê³ /": {  # Opus 4.5 ê¸°ë°˜
        "writer": "claude_opus",         # Opus 4.5 - VIP Writer
        "auditor": "claude_sonnet",      # Sonnet 4 - VIP Auditor
        "description": "VIP-AUDIT: Opus + Sonnet í¬ë¡œìŠ¤ì²´í¬"
    },
    "ìƒê°/": {  # GPT-5.2 Thinking ê¸°ë°˜
        "writer": "gpt_thinking",        # GPT-5.2 Thinking Extended
        "auditor": "claude_opus",        # Opus 4.5 - í¬ë¡œìŠ¤ì²´í¬
        "description": "VIP-THINKING: GPT-5.2 + Opus í¬ë¡œìŠ¤ì²´í¬"
    },
    "ê²€ìƒ‰/": {  # Perplexity ê¸°ë°˜
        "writer": "perplexity_sonar",    # Perplexity Sonar Pro
        "auditor": "gpt_5_mini",         # GPT-5 mini - íŒ©íŠ¸ì²´í¬
        "description": "RESEARCH: Perplexity + íŒ©íŠ¸ì²´í¬"
    },
}

# ìœ„ì›íšŒë³„ ëª¨ë¸ í• ë‹¹ (ì €ë ´í•œ ëª¨ë¸ ìœ„ì£¼, íƒ€ì´ë¸Œë ˆì´ì»¤ë§Œ ë¹„ì‹¼ ëª¨ë¸)
COUNCIL_MODEL_MAPPING = {
    "code": {
        "personas": {
            "skeptic": "gpt_5_mini",
            "perfectionist": "claude_haiku",    # Haiku ì—†ìœ¼ë©´ 4o-minië¡œ ëŒ€ì²´
            "pragmatist": "gpt_5_mini",
        },
        "tiebreaker": "claude_sonnet",           # ì˜ê²¬ ê°ˆë¦´ ë•Œ Sonnet
    },
    "strategy": {
        "personas": {
            "pessimist": "gpt_5_mini",
            "optimist": "claude_haiku",
            "devils_advocate": "gpt_5_mini",
        },
        "tiebreaker": "gpt_thinking",            # ì „ëµì€ GPT-5.2 Thinking
    },
    "security": {
        "personas": {
            "security_hawk": "claude_sonnet",    # ë³´ì•ˆì€ Sonnet í•„ìˆ˜
            "skeptic": "gpt_5_mini",
            "pessimist": "gpt_5_mini",
        },
        "tiebreaker": "claude_opus",             # ë³´ì•ˆ ìµœì¢…ì€ Opus
    },
    "deploy": {
        "personas": {
            "security_hawk": "claude_sonnet",
            "pessimist": "gpt_5_mini",
            "pragmatist": "gpt_5_mini",
            "perfectionist": "claude_haiku",
        },
        "tiebreaker": "claude_opus",             # ë°°í¬ ìµœì¢…ì€ CEO(Opus)
        "requires_ceo": True,
    },
    "mvp": {
        "personas": {
            "pragmatist": "gpt_5_mini",
            "optimist": "gpt_5_mini",
            "skeptic": "claude_haiku",
        },
        "tiebreaker": "claude_sonnet",
    },
}

# ë£¨í”„ ë¸Œë ˆì´ì»¤ ì„¤ì •
LOOP_BREAKER_CONFIG = {
    "MAX_STAGE_RETRY": 2,      # ê°™ì€ ë‹¨ê³„ ìµœëŒ€ ì¬ì‹œë„
    "MAX_TOTAL_STEPS": 8,      # ì „ì²´ ìµœëŒ€ ë‹¨ê³„
    "SIMILARITY_THRESHOLD": 0.85,  # ë°˜ë³µ ì‘ë‹µ ê°ì§€ (85% ìœ ì‚¬ë„)
    "ESCALATE_TO_CEO": True,   # ë£¨í”„ ê°ì§€ì‹œ CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜
}

# Haiku ëª¨ë¸ ì¶”ê°€ (ì €ë ´í•œ ìœ„ì›íšŒìš©) - config.pyì— ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
# ì´ì œ config.pyì— claude_haikuê°€ ì •ì˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì´ ë¸”ë¡ì€ í´ë°±ìš©ìœ¼ë¡œë§Œ ìœ ì§€

from src.infra.stream import get_stream
from src.core.router import get_router, route_message
from src.services import database as db
from src.services import executor
from src.services import rag
from src.services.agent_scorecard import get_scorecard


# =============================================================================
# LLM API Clients
# =============================================================================

THINKING_EXTEND_PREFIX = """
## THINKING EXTEND MODE ACTIVATED
You are operating in deep reasoning mode. Before providing any answer:

1. ANALYZE: Break down the problem into components
2. QUESTION: Identify assumptions and edge cases
3. EVALUATE: Consider alternative interpretations
4. SYNTHESIZE: Combine insights into coherent response

Do NOT skip reasoning steps. Prioritize correctness over brevity.
Think step-by-step internally before outputting your final structured response.

---

"""


def call_anthropic(model_config: ModelConfig, messages: list, system_prompt: str) -> str:
    """Anthropic API í˜¸ì¶œ"""
    try:
        import anthropic
        client = anthropic.Anthropic(api_key=os.getenv(model_config.api_key_env))

        response = client.messages.create(
            model=model_config.model_id,
            max_tokens=model_config.max_tokens,
            temperature=model_config.temperature,
            system=system_prompt,
            messages=messages
        )
        return response.content[0].text
    except Exception as e:
        return f"[Anthropic Error] {str(e)}"


def call_openai(model_config: ModelConfig, messages: list, system_prompt: str) -> str:
    """OpenAI API í˜¸ì¶œ"""
    try:
        import openai
        client = openai.OpenAI(api_key=os.getenv(model_config.api_key_env))

        if getattr(model_config, 'thinking_mode', False):
            system_prompt = THINKING_EXTEND_PREFIX + system_prompt

        full_messages = [{"role": "system", "content": system_prompt}]
        full_messages.extend(messages)

        # GPT-5 ê³„ì—´: temperature ì§€ì› ì•ˆí•¨, max_completion_tokens ì‚¬ìš©
        if model_config.model_id.startswith("gpt-5"):
            response = client.chat.completions.create(
                model=model_config.model_id,
                max_completion_tokens=model_config.max_tokens,
                # GPT-5ëŠ” temperature=1ë§Œ ì§€ì› (íŒŒë¼ë¯¸í„° ìƒëµ)
                messages=full_messages
            )
        else:
            response = client.chat.completions.create(
                model=model_config.model_id,
                max_tokens=model_config.max_tokens,
                temperature=model_config.temperature,
                messages=full_messages
            )
        return response.choices[0].message.content
    except Exception as e:
        return f"[OpenAI Error] {str(e)}"


def call_google(model_config: ModelConfig, messages: list, system_prompt: str) -> str:
    """Google Gemini API í˜¸ì¶œ"""
    try:
        if "gemini-3" in model_config.model_id:
            from google import genai
            client = genai.Client(api_key=os.getenv(model_config.api_key_env))

            contents = []
            for msg in messages:
                role = "user" if msg["role"] == "user" else "model"
                contents.append({"role": role, "parts": [{"text": msg["content"]}]})

            response = client.models.generate_content(
                model=model_config.model_id,
                contents=contents,
                config={
                    "system_instruction": system_prompt,
                    "temperature": model_config.temperature,
                    "max_output_tokens": model_config.max_tokens,
                }
            )
            return response.text
        else:
            import google.generativeai as genai
            genai.configure(api_key=os.getenv(model_config.api_key_env))

            model = genai.GenerativeModel(
                model_name=model_config.model_id,
                system_instruction=system_prompt
            )

            history = []
            for msg in messages[:-1]:
                role = "user" if msg["role"] == "user" else "model"
                history.append({"role": role, "parts": [msg["content"]]})

            chat = model.start_chat(history=history)
            response = chat.send_message(messages[-1]["content"])
            return response.text
    except Exception as e:
        return f"[Google Error] {str(e)}"


def call_llm(model_config: ModelConfig, messages: list, system_prompt: str) -> str:
    """LLM í˜¸ì¶œ ë¼ìš°í„°"""
    if model_config.provider == "anthropic":
        return call_anthropic(model_config, messages, system_prompt)
    elif model_config.provider == "openai":
        return call_openai(model_config, messages, system_prompt)
    elif model_config.provider == "google":
        return call_google(model_config, messages, system_prompt)
    else:
        return f"[Error] Unknown provider: {model_config.provider}"


def call_dual_engine(role: str, messages: list, system_prompt: str) -> str:
    """ë“€ì–¼ ì—”ì§„ í˜¸ì¶œ ë° ë³‘í•© (ë ˆê±°ì‹œ - config.py DUAL_ENGINES ì‚¬ìš©)"""
    config = DUAL_ENGINES.get(role)
    if not config:
        return f"[Error] Unknown dual engine role: {role}"

    response_1 = call_llm(config.engine_1, messages, system_prompt)
    response_2 = call_llm(config.engine_2, messages, system_prompt)

    if config.merge_strategy == "primary_fallback":
        if "[Error]" not in response_1:
            merged = f"""## {config.engine_1.name} (Primary)
{response_1}

---
## {config.engine_2.name} (Review)
{response_2}"""
        else:
            merged = response_2
    elif config.merge_strategy == "parallel":
        merged = f"""## {config.engine_1.name}
{response_1}

---
## {config.engine_2.name}
{response_2}"""
    else:
        merged = f"""## {config.engine_1.name}
{response_1}

---
## {config.engine_2.name}
{response_2}

---
**ë“€ì–¼ ì—”ì§„ ë¶„ì„ ì™„ë£Œ. ë‘ ê²°ê³¼ë¥¼ ë¹„êµ ê²€í† í•˜ì„¸ìš”.**"""

    stream = get_stream()
    stream.log_dual_engine(role, messages[-1]["content"], response_1, response_2, merged)

    return merged


# =============================================================================
# ë“€ì–¼ ì—”ì§„ V2 (Writer + Auditor íŒ¨í„´)
# =============================================================================

def call_dual_engine_v2(
    role: str,
    messages: list,
    system_prompt: str
) -> Tuple[str, Dict[str, Any]]:
    """
    ë“€ì–¼ ì—”ì§„ V2: Writer + Auditor íŒ¨í„´

    1ë‹¨ê³„: Writerê°€ ì´ˆì•ˆ ì‘ì„±
    2ë‹¨ê³„: Auditorê°€ ë¦¬ë·° ë° ìˆ˜ì • ì œì•ˆ
    3ë‹¨ê³„: ì˜ê²¬ ë¶ˆì¼ì¹˜ì‹œ ë³‘í•© ë˜ëŠ” ìœ„ì›íšŒ ì†Œì§‘

    Returns:
        (ìµœì¢… ì‘ë‹µ, ë©”íƒ€ë°ì´í„°)
    """
    if role not in DUAL_ENGINE_ROLES:
        # ë“€ì–¼ ì—”ì§„ ì—­í• ì´ ì•„ë‹ˆë©´ ë‹¨ì¼ ì—”ì§„ìœ¼ë¡œ í´ë°±
        return call_llm(MODELS.get("claude_sonnet", MODELS["claude_opus"]), messages, system_prompt), {"dual": False}

    config = DUAL_ENGINE_ROLES[role]
    writer_model = MODELS.get(config["writer"], MODELS["claude_sonnet"])
    auditor_model = MODELS.get(config["auditor"], MODELS["gpt_5_mini"])

    # 1ë‹¨ê³„: Writer ì´ˆì•ˆ ì‘ì„±
    print(f"[Dual-V2] {role} Writer ({writer_model.name}) ì‘ì—… ì¤‘...")
    writer_response = call_llm(writer_model, messages, system_prompt)

    if "[Error]" in writer_response:
        return writer_response, {"dual": True, "error": "writer_failed"}

    # 2ë‹¨ê³„: Auditor ë¦¬ë·°
    auditor_prompt = f"""ë‹¹ì‹ ì€ {role} ì‘ì—…ì˜ Auditor(ê°ì‚¬ê´€)ì…ë‹ˆë‹¤.

Writerê°€ ì‘ì„±í•œ ë‹¤ìŒ ê²°ê³¼ë¬¼ì„ ê²€í† í•˜ì„¸ìš”:

=== WRITER ê²°ê³¼ë¬¼ ===
{writer_response}
======================

ê²€í†  ê¸°ì¤€:
1. ë…¼ë¦¬ì  ì˜¤ë¥˜/ë²„ê·¸ í™•ì¸
2. ëˆ„ë½ëœ ì—£ì§€ì¼€ì´ìŠ¤ í™•ì¸
3. ë³´ì•ˆ ì·¨ì•½ì  í™•ì¸
4. ê°œì„  ì œì•ˆ

ì¶œë ¥ í˜•ì‹:
```yaml
verdict: "approve/revise/reject"
issues:
  - severity: "critical/high/medium/low"
    description: "ë¬¸ì œ ì„¤ëª…"
    fix: "ìˆ˜ì • ì œì•ˆ"
improvements:
  - "ê°œì„  ì‚¬í•­ 1"
  - "ê°œì„  ì‚¬í•­ 2"
final_comment: "ìµœì¢… ì½”ë©˜íŠ¸"
```
"""

    auditor_messages = messages.copy()
    auditor_messages.append({"role": "assistant", "content": writer_response})
    auditor_messages.append({"role": "user", "content": auditor_prompt})

    print(f"[Dual-V2] {role} Auditor ({auditor_model.name}) ë¦¬ë·° ì¤‘...")
    auditor_response = call_llm(auditor_model, auditor_messages, system_prompt)

    # ê²°ê³¼ ë³‘í•©
    merged_response = f"""## ğŸ“ Writer ({writer_model.name})
{writer_response}

---

## ğŸ” Auditor ({auditor_model.name})
{auditor_response}

---
âœ… **ë“€ì–¼ ì—”ì§„ ê²€í†  ì™„ë£Œ** ({config['description']})
"""

    # ë©”íƒ€ë°ì´í„°
    meta = {
        "dual": True,
        "writer_model": writer_model.name,
        "auditor_model": auditor_model.name,
        "role": role,
        "description": config["description"],
    }

    # ë¡œê·¸
    stream = get_stream()
    stream.log_dual_engine(role, messages[-1]["content"], writer_response, auditor_response, merged_response)

    return merged_response, meta


def call_vip_dual_engine(
    prefix: str,
    messages: list,
    system_prompt: str
) -> Tuple[str, Dict[str, Any]]:
    """
    VIP ë“€ì–¼ ì—”ì§„: CEO í”„ë¦¬í”½ìŠ¤ ê¸°ë°˜ VIP Writer + Auditor íŒ¨í„´

    - ìµœê³ / : Opus + Sonnet í¬ë¡œìŠ¤ì²´í¬
    - ìƒê°/ : GPT-5.2 Thinking + Opus í¬ë¡œìŠ¤ì²´í¬
    - ê²€ìƒ‰/ : Perplexity + 4o-mini íŒ©íŠ¸ì²´í¬

    Returns:
        (ìµœì¢… ì‘ë‹µ, ë©”íƒ€ë°ì´í„°)
    """
    if prefix not in VIP_DUAL_ENGINE:
        # VIP í”„ë¦¬í”½ìŠ¤ê°€ ì•„ë‹ˆë©´ ê¸°ë³¸ ëª¨ë¸ë¡œ í´ë°±
        return call_llm(MODELS.get("claude_opus", list(MODELS.values())[0]), messages, system_prompt), {"dual": False, "vip": False}

    config = VIP_DUAL_ENGINE[prefix]
    writer_model = MODELS.get(config["writer"])
    auditor_model = MODELS.get(config["auditor"])

    if not writer_model:
        print(f"[VIP-Dual] Writer ëª¨ë¸ {config['writer']} ì—†ìŒ, í´ë°±")
        writer_model = MODELS.get("claude_opus", list(MODELS.values())[0])

    if not auditor_model:
        print(f"[VIP-Dual] Auditor ëª¨ë¸ {config['auditor']} ì—†ìŒ, í´ë°±")
        auditor_model = MODELS.get("claude_sonnet", MODELS.get("gpt_5_mini"))

    # 1ë‹¨ê³„: VIP Writer ì‘ì—…
    print(f"[VIP-Dual] VIP Writer ({writer_model.name}) ì‘ì—… ì¤‘...")
    writer_response = call_llm(writer_model, messages, system_prompt)

    if "[Error]" in writer_response:
        return writer_response, {"dual": True, "vip": True, "error": "writer_failed"}

    # 2ë‹¨ê³„: VIP Auditor í¬ë¡œìŠ¤ì²´í¬
    auditor_prompt = f"""ë‹¹ì‹ ì€ VIP ë ˆë²¨ì˜ Auditor(ê°ì‚¬ê´€)ì…ë‹ˆë‹¤.

ë‹¤ë¥¸ VIP ëª¨ë¸ì´ ì‘ì„±í•œ ë‹¤ìŒ ê²°ê³¼ë¬¼ì„ í¬ë¡œìŠ¤ì²´í¬í•˜ì„¸ìš”:

=== VIP WRITER ê²°ê³¼ë¬¼ ===
{writer_response}
=========================

VIP ë ˆë²¨ ê²€í†  ê¸°ì¤€:
1. ë…¼ë¦¬ì  ì™„ê²°ì„± ë° ì •í™•ë„
2. ëˆ„ë½ëœ ê´€ì /ì—£ì§€ì¼€ì´ìŠ¤
3. CEO ì˜ì‚¬ê²°ì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
4. ë¦¬ìŠ¤í¬ ìš”ì†Œ í™•ì¸
5. ê°œì„ /ë³´ì™„ ì œì•ˆ

ì¶œë ¥ í˜•ì‹:
```yaml
verdict: "approve/revise/escalate"
confidence: 0-100
key_findings:
  - "í•µì‹¬ ë°œê²¬ 1"
  - "í•µì‹¬ ë°œê²¬ 2"
concerns:
  - severity: "critical/high/medium/low"
    description: "ìš°ë ¤ ì‚¬í•­"
recommendations:
  - "ê¶Œì¥ ì‚¬í•­ 1"
  - "ê¶Œì¥ ì‚¬í•­ 2"
final_assessment: "ìµœì¢… í‰ê°€ (2-3ë¬¸ì¥)"
```
"""

    auditor_messages = messages.copy()
    auditor_messages.append({"role": "assistant", "content": writer_response})
    auditor_messages.append({"role": "user", "content": auditor_prompt})

    print(f"[VIP-Dual] VIP Auditor ({auditor_model.name}) í¬ë¡œìŠ¤ì²´í¬ ì¤‘...")
    auditor_response = call_llm(auditor_model, auditor_messages, system_prompt)

    # ê²°ê³¼ ë³‘í•©
    merged_response = f"""## ğŸ“ VIP Writer ({writer_model.name})
{writer_response}

---

## ğŸ” VIP Auditor ({auditor_model.name})
{auditor_response}

---
âœ… **VIP ë“€ì–¼ ì—”ì§„ ê²€í†  ì™„ë£Œ** ({config['description']})
"""

    # ë©”íƒ€ë°ì´í„°
    meta = {
        "dual": True,
        "vip": True,
        "prefix": prefix,
        "writer_model": writer_model.name,
        "auditor_model": auditor_model.name,
        "description": config["description"],
    }

    # ë¡œê·¸
    stream = get_stream()
    stream.log_dual_engine(f"VIP-{prefix}", messages[-1]["content"], writer_response, auditor_response, merged_response)

    return merged_response, meta


# =============================================================================
# ìœ„ì›íšŒ í˜¸ì¶œ (Council Integration)
# =============================================================================

async def call_council_llm(
    system_prompt: str,
    user_message: str,
    temperature: float,
    persona_id: str = None,
    council_type: str = None
) -> str:
    """
    ìœ„ì›íšŒ í˜ë¥´ì†Œë‚˜ìš© LLM í˜¸ì¶œ

    COUNCIL_MODEL_MAPPINGì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ
    """
    # ëª¨ë¸ ì„ íƒ ë¡œì§
    model_key = "gpt_5_mini"  # ê¸°ë³¸ê°’

    if council_type and persona_id:
        mapping = COUNCIL_MODEL_MAPPING.get(council_type, {})
        personas = mapping.get("personas", {})
        model_key = personas.get(persona_id, "gpt_5_mini")

    model_config = MODELS.get(model_key)
    if not model_config:
        model_config = MODELS["gpt_5_mini"] if "gpt_5_mini" in MODELS else list(MODELS.values())[0]

    # temperature ì˜¤ë²„ë¼ì´ë“œ
    original_temp = model_config.temperature
    model_config.temperature = temperature

    messages = [{"role": "user", "content": user_message}]
    response = call_llm(model_config, messages, system_prompt)

    # temperature ë³µì›
    model_config.temperature = original_temp

    return response


def init_council_with_llm():
    """ìœ„ì›íšŒì— LLM Caller ì£¼ì…"""
    from src.infra.council import get_council

    council = get_council()

    async def council_llm_caller(
        system_prompt: str,
        user_message: str,
        temperature: float,
        persona_id: str = None,
        council_type: str = None
    ) -> str:
        """ìœ„ì›íšŒ LLM í˜¸ì¶œ (ëª¨ë¸ ë§¤í•‘ ì§€ì›)"""
        # ëª¨ë¸ ì„ íƒ ë¡œì§
        model_key = "gpt_5_mini"  # ê¸°ë³¸ê°’

        if council_type and persona_id:
            mapping = COUNCIL_MODEL_MAPPING.get(council_type, {})
            personas = mapping.get("personas", {})
            model_key = personas.get(persona_id, "gpt_5_mini")

        model_config = MODELS.get(model_key)
        if not model_config:
            model_config = MODELS.get("gpt_5_mini", list(MODELS.values())[0])

        print(f"[Council] {persona_id} â†’ {model_config.name}")

        # ë™ê¸° í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        def sync_call():
            # temperature ì˜¤ë²„ë¼ì´ë“œ
            original_temp = model_config.temperature
            model_config.temperature = temperature

            messages = [{"role": "user", "content": user_message}]
            response = call_llm(model_config, messages, system_prompt)

            # temperature ë³µì›
            model_config.temperature = original_temp
            return response

        return await asyncio.get_event_loop().run_in_executor(None, sync_call)

    council.set_llm_caller(council_llm_caller)
    print("[Council] LLM Caller ì£¼ì… ì™„ë£Œ (ëª¨ë¸ ë§¤í•‘ í™œì„±í™”)")
    return council


def should_convene_council(agent_role: str, response: str, context: Dict = None) -> Optional[str]:
    """
    ìœ„ì›íšŒ ìë™ ì†Œì§‘ ì¡°ê±´ íŒë‹¨

    Returns:
        ìœ„ì›íšŒ ìœ í˜• ë˜ëŠ” None
    """
    context = context or {}

    # 1. ì „ëµ ë³€ê²½ ê°ì§€
    strategy_keywords = ["ì „ëµ", "strategy", "ë°©í–¥", "decision", "ê²°ì •", "plan"]
    if agent_role == "strategist" or any(kw in response.lower() for kw in strategy_keywords):
        if len(response) > 500:  # ê¸´ ì „ëµ ì‘ë‹µ
            return "strategy"

    # 2. ì½”ë“œ íŒ¨ì¹˜ ê°ì§€
    code_keywords = ["```python", "```javascript", "```typescript", "def ", "class ", "function "]
    if agent_role == "coder" or any(kw in response for kw in code_keywords):
        if "def " in response or "class " in response:
            return "code"

    # 3. ë³´ì•ˆ ê´€ë ¨ ê°ì§€
    security_keywords = ["password", "api_key", "secret", "token", "auth", "ë³´ì•ˆ", "ì·¨ì•½ì "]
    if any(kw in response.lower() for kw in security_keywords):
        return "security"

    # 4. ë°°í¬ ê´€ë ¨ ê°ì§€
    deploy_keywords = ["deploy", "ë°°í¬", "production", "release", "push"]
    if any(kw in response.lower() for kw in deploy_keywords):
        return "deploy"

    # 5. ë“€ì–¼ ì—”ì§„ ì˜ê²¬ ë¶ˆì¼ì¹˜ ê°ì§€ (Auditorê°€ reject íŒì •)
    if "verdict: reject" in response.lower() or "verdict: revise" in response.lower():
        if agent_role == "coder":
            return "code"
        elif agent_role == "strategist":
            return "strategy"

    return None


async def convene_council_async(
    council_type: str,
    content: str,
    context: str = ""
) -> Dict:
    """
    ë¹„ë™ê¸° ìœ„ì›íšŒ ì†Œì§‘

    Returns:
        íŒì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    from src.infra.council import get_council, Verdict

    council = get_council()

    # LLM Callerê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì´ˆê¸°í™”
    if council.llm_caller is None:
        init_council_with_llm()

    print(f"[Council] {council_type.upper()} ìœ„ì›íšŒ ì†Œì§‘ ì¤‘...")
    verdict = await council.convene(council_type, content, context)

    result = {
        "council_type": council_type,
        "verdict": verdict.verdict.value,
        "average_score": verdict.average_score,
        "score_std": verdict.score_std,
        "requires_ceo": verdict.requires_ceo,
        "summary": verdict.summary,
        "judges": [
            {
                "persona": j.persona_name,
                "icon": j.icon,
                "score": j.score,
                "reasoning": j.reasoning,
            }
            for j in verdict.judges
        ]
    }

    print(f"[Council] íŒì •: {verdict.verdict.value} (í‰ê·  {verdict.average_score}/10)")
    return result


def convene_council_sync(council_type: str, content: str, context: str = "") -> Dict:
    """ë™ê¸° ë²„ì „ ìœ„ì›íšŒ ì†Œì§‘"""
    return asyncio.run(convene_council_async(council_type, content, context))


# =============================================================================
# ë£¨í”„ ë¸Œë ˆì´ì»¤ (Loop Breaker)
# =============================================================================

class LoopBreaker:
    """
    ì—ì´ì „íŠ¸ ë£¨í”„ ê°ì§€ ë° ì°¨ë‹¨

    - MAX_STAGE_RETRY: ê°™ì€ ë‹¨ê³„ ìµœëŒ€ ì¬ì‹œë„
    - MAX_TOTAL_STEPS: ì „ì²´ ìµœëŒ€ ë‹¨ê³„
    - ë°˜ë³µ ì‘ë‹µ ê°ì§€ (ìœ ì‚¬ë„ ê¸°ë°˜)
    - CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜
    """

    def __init__(self):
        self.step_count = 0
        self.stage_retries: Dict[str, int] = {}
        self.response_history: list = []
        self.is_broken = False
        self.break_reason = None

    def reset(self):
        """ë¸Œë ˆì´ì»¤ ì´ˆê¸°í™”"""
        self.step_count = 0
        self.stage_retries = {}
        self.response_history = []
        self.is_broken = False
        self.break_reason = None

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ Jaccard)"""
        if not text1 or not text2:
            return 0.0

        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1 & words2
        union = words1 | words2

        return len(intersection) / len(union)

    def check_and_update(self, stage: str, response: str) -> Tuple[bool, Optional[str]]:
        """
        ë£¨í”„ ì²´í¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸

        Args:
            stage: í˜„ì¬ ë‹¨ê³„ (ì˜ˆ: "coder", "qa", "strategist")
            response: ì—ì´ì „íŠ¸ ì‘ë‹µ

        Returns:
            (should_break, break_reason): ì¤‘ë‹¨í•´ì•¼ í•˜ë©´ Trueì™€ ì‚¬ìœ 
        """
        config = LOOP_BREAKER_CONFIG

        # 1. ì „ì²´ ë‹¨ê³„ ìˆ˜ ì²´í¬
        self.step_count += 1
        if self.step_count > config["MAX_TOTAL_STEPS"]:
            self.is_broken = True
            self.break_reason = f"MAX_TOTAL_STEPS ì´ˆê³¼ ({self.step_count}/{config['MAX_TOTAL_STEPS']})"
            return True, self.break_reason

        # 2. ê°™ì€ ë‹¨ê³„ ì¬ì‹œë„ ì²´í¬
        self.stage_retries[stage] = self.stage_retries.get(stage, 0) + 1
        if self.stage_retries[stage] > config["MAX_STAGE_RETRY"]:
            self.is_broken = True
            self.break_reason = f"MAX_STAGE_RETRY ì´ˆê³¼: {stage} ({self.stage_retries[stage]}íšŒ)"
            return True, self.break_reason

        # 3. ë°˜ë³µ ì‘ë‹µ ê°ì§€
        for prev_response in self.response_history[-3:]:  # ìµœê·¼ 3ê°œì™€ ë¹„êµ
            similarity = self._calculate_similarity(response, prev_response)
            if similarity > config["SIMILARITY_THRESHOLD"]:
                self.is_broken = True
                self.break_reason = f"ë°˜ë³µ ì‘ë‹µ ê°ì§€ (ìœ ì‚¬ë„: {similarity:.2%})"
                return True, self.break_reason

        # 4. ì‘ë‹µ íˆìŠ¤í† ë¦¬ ì €ì¥
        self.response_history.append(response[:1000])  # ì²˜ìŒ 1000ìë§Œ

        return False, None

    def get_escalation_message(self) -> str:
        """CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜ ë©”ì‹œì§€ ìƒì„±"""
        return f"""
âš ï¸ **ë£¨í”„ ë¸Œë ˆì´ì»¤ ë°œë™**

**ì‚¬ìœ **: {self.break_reason}
**ì§„í–‰ ë‹¨ê³„**: {self.step_count}íšŒ
**ë‹¨ê³„ë³„ ì¬ì‹œë„**: {dict(self.stage_retries)}

---

**ê¶Œì¥ ì¡°ì¹˜**:
1. í˜„ì¬ ì‘ì—…ì„ ìˆ˜ë™ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”
2. ìš”ì²­ì„ ë” ëª…í™•í•˜ê²Œ ì¬ì •ì˜í•˜ì„¸ìš”
3. ì‘ì—… ë²”ìœ„ë¥¼ ì¶•ì†Œí•˜ì„¸ìš”

**ìë™ ì¡°ì¹˜**: ë£¨í”„ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

    def should_escalate_to_ceo(self) -> bool:
        """CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€"""
        return self.is_broken and LOOP_BREAKER_CONFIG.get("ESCALATE_TO_CEO", True)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_loop_breaker: Optional[LoopBreaker] = None


def get_loop_breaker() -> LoopBreaker:
    """LoopBreaker ì‹±ê¸€í†¤"""
    global _loop_breaker
    if _loop_breaker is None:
        _loop_breaker = LoopBreaker()
    return _loop_breaker


def check_loop(stage: str, response: str) -> Tuple[bool, Optional[str]]:
    """ë£¨í”„ ì²´í¬ í—¬í¼ í•¨ìˆ˜"""
    return get_loop_breaker().check_and_update(stage, response)


# =============================================================================
# Agent Call
# =============================================================================

def strip_ceo_prefix(message: str) -> tuple[str, str]:
    """
    CEO í”„ë¦¬í”½ìŠ¤ ì œê±° ë° ì‹¤ì œ ë©”ì‹œì§€ ì¶”ì¶œ
    [PROJECT: xxx] ë˜í¼ê°€ ìˆì–´ë„ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬

    Returns:
        (ì‹¤ì œ ë©”ì‹œì§€, ì‚¬ìš©ëœ í”„ë¦¬í”½ìŠ¤ or None)

    ì˜ˆì‹œ:
        "ìµœê³ / ì½”ë“œ ë¦¬ë·°í•´ì¤˜" â†’ ("ì½”ë“œ ë¦¬ë·°í•´ì¤˜", "ìµœê³ /")
        "[PROJECT: test]\nìµœê³ / ë¦¬ë·°í•´ì¤˜" â†’ ("[PROJECT: test]\në¦¬ë·°í•´ì¤˜", "ìµœê³ /")
        "ìƒê°/ ì™œ ì•ˆë ê¹Œ?" â†’ ("ì™œ ì•ˆë ê¹Œ?", "ìƒê°/")
        "ê²€ìƒ‰/ ìµœì‹  ë²„ì „" â†’ ("ìµœì‹  ë²„ì „", "ê²€ìƒ‰/")
        "ì¼ë°˜ ë©”ì‹œì§€" â†’ ("ì¼ë°˜ ë©”ì‹œì§€", None)
    """
    prefixes = ["ìµœê³ /", "ìƒê°/", "ê²€ìƒ‰/"]

    # Case 1: ì§ì ‘ í”„ë¦¬í”½ìŠ¤ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
    for prefix in prefixes:
        if message.startswith(prefix):
            actual_message = message[len(prefix):].lstrip()
            return actual_message, prefix

    # Case 2: [PROJECT: xxx]\n ë˜í¼ê°€ ìˆëŠ” ê²½ìš°
    if message.startswith("[PROJECT:"):
        lines = message.split("\n", 1)
        if len(lines) > 1:
            project_line = lines[0]  # "[PROJECT: xxx]"
            content_line = lines[1]   # "ìµœê³ / ì‹¤ì œ ë©”ì‹œì§€"

            for prefix in prefixes:
                if content_line.startswith(prefix):
                    # í”„ë¦¬í”½ìŠ¤ ì œê±° í›„ [PROJECT:] ìœ ì§€
                    actual_content = content_line[len(prefix):].lstrip()
                    return f"{project_line}\n{actual_content}", prefix

    return message, None


def extract_project_from_message(message: str) -> tuple[str, str]:
    """
    [PROJECT: xxx] íƒœê·¸ì—ì„œ í”„ë¡œì íŠ¸ëª… ì¶”ì¶œ

    Returns:
        (project_name, message_without_project_tag)

    ì˜ˆì‹œ:
        "[PROJECT: test]\nì•ˆë…•" â†’ ("test", "ì•ˆë…•")
        "ê·¸ëƒ¥ ë©”ì‹œì§€" â†’ (None, "ê·¸ëƒ¥ ë©”ì‹œì§€")
    """
    import re
    match = re.match(r'\[PROJECT:\s*([^\]]+)\]\s*\n?(.*)', message, re.DOTALL)
    if match:
        project = match.group(1).strip()
        remaining = match.group(2).strip()
        return project, remaining
    return None, message


def call_agent(
    message: str,
    agent_role: str,
    auto_execute: bool = True,
    use_translation: bool = True,
    use_router: bool = True,
    return_meta: bool = False,
    use_dual_engine: bool = True,   # ë“€ì–¼ ì—”ì§„ ì‚¬ìš© ì—¬ë¶€
    auto_council: bool = True,      # ìœ„ì›íšŒ ìë™ ì†Œì§‘ ì—¬ë¶€
) -> str | tuple[str, dict]:
    """
    ì‹¤ì œ LLM í˜¸ì¶œ + [EXEC] íƒœê·¸ ìë™ ì‹¤í–‰ + RAG ì»¨í…ìŠ¤íŠ¸ ì£¼ì… + ë²ˆì—­ + ìŠ¤ì½”ì–´ì¹´ë“œ ë¡œê¹…

    CEO í”„ë¦¬í”½ìŠ¤ ì§€ì›:
    - ìµœê³ / : VIP-AUDIT (Opus 4.5) ê°•ì œ
    - ìƒê°/ : VIP-THINKING (GPT-5.2 Thinking Extend) ê°•ì œ
    - ê²€ìƒ‰/ : RESEARCH (Perplexity) ê°•ì œ

    Args:
        return_meta: Trueì´ë©´ (response, meta_dict) íŠœí”Œ ë°˜í™˜

    Returns:
        str ë˜ëŠ” (str, dict): response ë˜ëŠ” (response, model_meta)
    """
    from src.core.session_state import get_current_session

    current_session_id = get_current_session()
    start_time = time_module.time()

    # ë””ë²„ê·¸: ì…ë ¥ ë©”ì‹œì§€ í™•ì¸
    import sys
    sys.stderr.write(f"[DEBUG-INPUT] message[:50]={message[:50] if len(message) > 50 else message}\n")
    sys.stderr.write(f"[DEBUG-INPUT] message.startswith('ìµœê³ /')={message.startswith('ìµœê³ /')}\n")
    sys.stderr.flush()

    # [PROJECT: xxx] íƒœê·¸ì—ì„œ í”„ë¡œì íŠ¸ ì¶”ì¶œ
    current_project, message_without_project = extract_project_from_message(message)
    if current_project:
        print(f"[Project] Detected: {current_project}")

    # CEO í”„ë¦¬í”½ìŠ¤ ì²´í¬ (ë¼ìš°íŒ…ìš© ì›ë³¸ ìœ ì§€)
    actual_message, used_prefix = strip_ceo_prefix(message)

    router = get_router()
    routing = route_message(message, agent_role)  # í”„ë¦¬í”½ìŠ¤ í¬í•¨ëœ ì›ë³¸ìœ¼ë¡œ ë¼ìš°íŒ…

    # ëª¨ë¸ ë©”íƒ€ ì •ë³´ ìˆ˜ì§‘
    model_meta = {
        'model_name': routing.model_spec.name,
        'model_id': routing.model_spec.model_id,
        'tier': routing.model_tier,
        'reason': routing.reason,
        'provider': routing.model_spec.provider,
        'ceo_prefix': used_prefix,
    }

    # í”„ë¦¬í”½ìŠ¤ ì‚¬ìš© ì‹œ ë¡œê·¸ í‘œì‹œ
    if used_prefix:
        print(f"[CEO Prefix] '{used_prefix}' detected â†’ VIP mode activated")

    print(f"[Router] {agent_role} â†’ {routing.model_tier.upper()} ({routing.model_spec.name})")
    print(f"[Router] Reason: {routing.reason}")

    system_prompt = get_system_prompt(agent_role)
    if not system_prompt:
        return f"[Error] Unknown agent role: {agent_role}"

    # í”„ë¦¬í”½ìŠ¤ ì œê±°ëœ ì‹¤ì œ ë©”ì‹œì§€ ì‚¬ìš©
    agent_message = actual_message
    if use_translation and rag.is_korean(actual_message):
        agent_message = rag.translate_for_agent(actual_message)
        print(f"[Translate] CEOâ†’Agent: {len(actual_message)}ì â†’ {len(agent_message)}ì")

    if agent_role == "pm":
        try:
            rag_context = rag.build_context(
                agent_message,
                project=current_project,  # í”„ë¡œì íŠ¸ë³„ RAG í•„í„°ë§
                top_k=5,
                use_gemini=True,
                language="en"
            )
            if rag_context:
                system_prompt = system_prompt + "\n\n" + rag_context
                print(f"[RAG] Context injected ({current_project or 'all'}): {len(rag_context)} chars")
        except Exception as e:
            print(f"[RAG] Context injection failed: {e}")

    messages = []
    if current_session_id:
        db_messages = db.get_messages(current_session_id)
        for msg in db_messages:
            if msg.get('agent') == agent_role:
                messages.append({
                    "role": msg['role'],
                    "content": msg['content']
                })

    messages.append({"role": "user", "content": agent_message})

    # ë“€ì–¼ ì—”ì§„ ë©”íƒ€ë°ì´í„°
    dual_meta = {"dual": False}
    council_result = None

    # ë””ë²„ê·¸: VIP ì¡°ê±´ ì²´í¬ (flush=Trueë¡œ ì¦‰ì‹œ ì¶œë ¥, stderrë¡œë„ ì¶œë ¥)
    import sys
    debug_msg = f"[DEBUG-VIP] use_dual_engine={use_dual_engine}, used_prefix='{used_prefix}', prefix_in_dict={used_prefix in VIP_DUAL_ENGINE if used_prefix else 'N/A'}"
    print(debug_msg, flush=True)
    sys.stderr.write(debug_msg + "\n")
    sys.stderr.flush()

    debug_msg2 = f"[DEBUG-VIP] VIP_DUAL_ENGINE keys: {list(VIP_DUAL_ENGINE.keys())}"
    print(debug_msg2, flush=True)
    sys.stderr.write(debug_msg2 + "\n")
    sys.stderr.flush()

    # =========================================================================
    # VIP ë“€ì–¼ ì—”ì§„ ëª¨ë“œ (CEO í”„ë¦¬í”½ìŠ¤ ì‚¬ìš© ì‹œ)
    # =========================================================================
    if use_dual_engine and used_prefix and used_prefix in VIP_DUAL_ENGINE:
        print(f"[VIP-Dual] {used_prefix} VIP ë“€ì–¼ ì—”ì§„ ëª¨ë“œ í™œì„±í™”")
        response, dual_meta = call_vip_dual_engine(used_prefix, messages, system_prompt)

        # VIP ëª¨ë“œì—ì„œë„ ìœ„ì›íšŒ ìë™ ì†Œì§‘ ì²´í¬
        if auto_council:
            council_type = should_convene_council(agent_role, response)
            if council_type:
                print(f"[Council] VIP ìë™ ì†Œì§‘ íŠ¸ë¦¬ê±°: {council_type}")
                try:
                    council_result = convene_council_sync(council_type, response, agent_message)
                    model_meta['council'] = council_result

                    # ìœ„ì›íšŒ ê²°ê³¼ë¥¼ ì‘ë‹µì— ì¶”ê°€
                    response += f"""

---

## ğŸ›ï¸ {council_type.upper()} ìœ„ì›íšŒ íŒì •

{council_result['summary']}

**ìƒì„¸ ì ìˆ˜:**
"""
                    for judge in council_result['judges']:
                        response += f"- {judge['icon']} {judge['persona']}: {judge['score']}/10 - {judge['reasoning'][:100]}...\n"

                except Exception as e:
                    print(f"[Council] ì†Œì§‘ ì‹¤íŒ¨: {e}")

        stream = get_stream()
        stream.log("ceo", agent_role, "request", agent_message)
        stream.log(agent_role, "ceo", "response", response)

    # =========================================================================
    # ì¼ë°˜ ë“€ì–¼ ì—”ì§„ V2 ì‚¬ìš© (use_dual_engine=Trueì´ê³  ì—­í• ì´ ì§€ì›ë˜ëŠ” ê²½ìš°)
    # =========================================================================
    elif use_dual_engine and agent_role in DUAL_ENGINE_ROLES and not used_prefix:
        print(f"[Dual-V2] {agent_role} ë“€ì–¼ ì—”ì§„ ëª¨ë“œ í™œì„±í™”")
        response, dual_meta = call_dual_engine_v2(agent_role, messages, system_prompt)

        # ìœ„ì›íšŒ ìë™ ì†Œì§‘ ì²´í¬
        if auto_council:
            council_type = should_convene_council(agent_role, response)
            if council_type:
                print(f"[Council] ìë™ ì†Œì§‘ íŠ¸ë¦¬ê±°: {council_type}")
                try:
                    council_result = convene_council_sync(council_type, response, agent_message)
                    model_meta['council'] = council_result

                    # ìœ„ì›íšŒ ê²°ê³¼ë¥¼ ì‘ë‹µì— ì¶”ê°€
                    response += f"""

---

## ğŸ›ï¸ {council_type.upper()} ìœ„ì›íšŒ íŒì •

{council_result['summary']}

**ìƒì„¸ ì ìˆ˜:**
"""
                    for judge in council_result['judges']:
                        response += f"- {judge['icon']} {judge['persona']}: {judge['score']}/10 - {judge['reasoning'][:100]}...\n"

                except Exception as e:
                    print(f"[Council] ì†Œì§‘ ì‹¤íŒ¨: {e}")

        stream = get_stream()
        stream.log("ceo", agent_role, "request", agent_message)
        stream.log(agent_role, "ceo", "response", response)

    # =========================================================================
    # ë ˆê±°ì‹œ ë¼ìš°í„° ëª¨ë“œ (ë“€ì–¼ ì—”ì§„ ë¹„í™œì„±í™” ì‹œ)
    # =========================================================================
    elif use_router:
        response = router.call_model(routing, messages, system_prompt)
        print(f"[Router] Called: {routing.model_spec.name}")

        stream = get_stream()
        stream.log("ceo", agent_role, "request", agent_message)
        stream.log(agent_role, "ceo", "response", response)

        # ë¼ìš°í„° ëª¨ë“œì—ì„œë„ ìœ„ì›íšŒ ìë™ ì†Œì§‘ ì²´í¬
        if auto_council:
            council_type = should_convene_council(agent_role, response)
            if council_type:
                print(f"[Council] ìë™ ì†Œì§‘ íŠ¸ë¦¬ê±°: {council_type}")
                try:
                    council_result = convene_council_sync(council_type, response, agent_message)
                    model_meta['council'] = council_result

                    response += f"""

---

## ğŸ›ï¸ {council_type.upper()} ìœ„ì›íšŒ íŒì •

{council_result['summary']}
"""
                except Exception as e:
                    print(f"[Council] ì†Œì§‘ ì‹¤íŒ¨: {e}")

    # =========================================================================
    # ë ˆê±°ì‹œ ëª¨ë“œ (use_router=False)
    # =========================================================================
    else:
        if agent_role in DUAL_ENGINES:
            response = call_dual_engine(agent_role, messages, system_prompt)
        else:
            model_config = SINGLE_ENGINES.get(agent_role)
            if model_config:
                response = call_llm(model_config, messages, system_prompt)
                stream = get_stream()
                stream.log("ceo", agent_role, "request", agent_message)
                stream.log(agent_role, "ceo", "response", response)
            else:
                return f"[Error] No engine configured for: {agent_role}"

    # ë“€ì–¼ ì—”ì§„ ë©”íƒ€ ì •ë³´ ë³‘í•©
    model_meta['dual_engine'] = dual_meta

    if auto_execute and agent_role in ["coder", "pm"]:
        exec_results = executor.execute_all(response)
        if exec_results:
            exec_output = executor.format_results(exec_results)

            if agent_role == "pm":
                followup_prompt = f"""## EXEC ì‹¤í–‰ ê²°ê³¼

ë‹¤ìŒì€ ë°©ê¸ˆ ìš”ì²­í•œ ëª…ë ¹ì–´ë“¤ì˜ ì‹¤í–‰ ê²°ê³¼ì…ë‹ˆë‹¤:

{exec_output}

---

ìœ„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•´ì£¼ì„¸ìš”:
1. í•µì‹¬ ë°œê²¬ ì‚¬í•­ (ì´ëª¨ì§€ í¬í•¨)
2. ë‹¤ìŒ ì•¡ì…˜ ì œì•ˆ (ìˆë‹¤ë©´)
3. ì£¼ì˜ì ì´ë‚˜ ë¦¬ìŠ¤í¬ (ìˆë‹¤ë©´)

ê°„ê²°í•˜ê²Œ í•œê¸€ë¡œ ë³´ê³ í•´ì£¼ì„¸ìš”."""

                analysis_response = call_agent(
                    followup_prompt,
                    agent_role,
                    auto_execute=False,
                    use_translation=False
                )
                response += f"\n\n---\n\n## EXEC ê²°ê³¼ ë¶„ì„\n\n{analysis_response}"
            else:
                response += exec_output

    if use_translation and not rag.is_korean(response):
        response = rag.translate_for_ceo(response)
        print(f"[Translate] Agentâ†’CEO: í•œêµ­ì–´ë¡œ ë²ˆì—­ ì™„ë£Œ")

    try:
        elapsed_ms = int((time_module.time() - start_time) * 1000)
        scorecard = get_scorecard()

        if use_router:
            model_name = routing.model_spec.model_id
            engine_type = f"router_{routing.model_tier}"
        elif agent_role in DUAL_ENGINES:
            model_name = DUAL_ENGINES[agent_role].engine_1.model_id
            engine_type = "dual"
        elif agent_role in SINGLE_ENGINES:
            model_name = SINGLE_ENGINES[agent_role].model_id
            engine_type = "single"
        else:
            model_name = "unknown"
            engine_type = "unknown"

        task_type_map = {
            'excavator': 'analysis',
            'coder': 'code',
            'strategist': 'strategy',
            'qa': 'test',
            'analyst': 'analysis',
            'researcher': 'research',
            'pm': 'orchestration'
        }

        scorecard.log_task(
            session_id=current_session_id or "no_session",
            task_id=f"task_{int(time_module.time())}",
            role=agent_role,
            engine=engine_type,
            model=model_name,
            task_type=task_type_map.get(agent_role, 'general'),
            task_summary=message[:100],
            input_tokens=len(message.split()) * 2,
            output_tokens=len(response.split()) * 2,
            latency_ms=elapsed_ms
        )
        print(f"[Scorecard] Logged: {agent_role} â†’ {model_name} ({elapsed_ms}ms)")

        # ë©”íƒ€ ì •ë³´ì— ì¶”ê°€ ë°ì´í„° ì—…ë°ì´íŠ¸
        model_meta['latency_ms'] = elapsed_ms
    except Exception as e:
        print(f"[Scorecard] Error: {e}")

    if return_meta:
        return response, model_meta
    return response


def process_call_tags(pm_response: str, use_loop_breaker: bool = True) -> list:
    """
    PM ì‘ë‹µì—ì„œ [CALL:agent] íƒœê·¸ë¥¼ ì²˜ë¦¬

    Args:
        pm_response: PM ì‘ë‹µ í…ìŠ¤íŠ¸
        use_loop_breaker: ë£¨í”„ ë¸Œë ˆì´ì»¤ ì‚¬ìš© ì—¬ë¶€

    Returns:
        ì—ì´ì „íŠ¸ í˜¸ì¶œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    calls = executor.extract_call_info(pm_response)
    results = []

    # ë£¨í”„ ë¸Œë ˆì´ì»¤ ì´ˆê¸°í™” (ìƒˆ íƒœìŠ¤í¬ ì‹œì‘)
    if use_loop_breaker:
        loop_breaker = get_loop_breaker()
        # ìƒˆ PM í˜¸ì¶œì´ë©´ ë¦¬ì…‹í•˜ì§€ ì•ŠìŒ (ì—°ì† ì‘ì—… ì¶”ì )

    for call in calls:
        agent = call['agent']
        message = call['message']

        # ë£¨í”„ ë¸Œë ˆì´ì»¤ ì²´í¬
        if use_loop_breaker:
            should_break, break_reason = check_loop(agent, message)
            if should_break:
                print(f"[LoopBreaker] ğŸ›‘ ë£¨í”„ ê°ì§€: {break_reason}")
                escalation_msg = get_loop_breaker().get_escalation_message()

                results.append({
                    'agent': 'loop_breaker',
                    'message': break_reason,
                    'response': escalation_msg,
                    'is_break': True
                })

                # CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜
                if get_loop_breaker().should_escalate_to_ceo():
                    print("[LoopBreaker] âš ï¸ CEO ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”")

                break  # ë” ì´ìƒ ì—ì´ì „íŠ¸ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ

        print(f"[CALL] PM â†’ {agent}: {message[:100]}...")

        response = call_agent(message, agent, auto_execute=True, use_translation=False)

        # ì‘ë‹µ ê¸°ë°˜ ë£¨í”„ ì²´í¬
        if use_loop_breaker:
            should_break, break_reason = check_loop(f"{agent}_response", response)
            if should_break:
                print(f"[LoopBreaker] ğŸ›‘ ë°˜ë³µ ì‘ë‹µ ê°ì§€: {break_reason}")
                response += f"\n\n---\n\nâš ï¸ **ë£¨í”„ ë¸Œë ˆì´ì»¤ ê²½ê³ **: {break_reason}"

        results.append({
            'agent': agent,
            'message': message,
            'response': response
        })

        print(f"[CALL] {agent} ì™„ë£Œ: {len(response)}ì")

    return results


def build_call_results_prompt(call_results: list) -> str:
    """í•˜ìœ„ ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ PMì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    prompt = "í•˜ìœ„ ì—ì´ì „íŠ¸ë“¤ì˜ ì‹¤í–‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•´ì£¼ì„¸ìš”.\n\n"

    for i, result in enumerate(call_results, 1):
        prompt += f"## {i}. {result['agent'].upper()} ì‘ë‹µ\n"
        prompt += f"**ìš”ì²­:** {result['message'][:200]}...\n\n"
        prompt += f"**ê²°ê³¼:**\n{result['response']}\n\n"
        prompt += "---\n\n"

    prompt += "ìœ„ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ CEOì—ê²Œ í•œê¸€ë¡œ ë³´ê³ í•´ì£¼ì„¸ìš”."
    return prompt


def mock_agent_response(message: str, agent_role: str) -> str:
    """Mock ì‘ë‹µ (í…ŒìŠ¤íŠ¸ìš©)"""
    responses = {
        'pm': f"""```yaml
sprint_plan:
  do:
    - "ìš”ì²­ ë¶„ì„: {message}"
    - "ì„¸ë¶€ íƒœìŠ¤í¬ ë¶„í•´"
    - "ì—ì´ì „íŠ¸ í• ë‹¹"
  dont:
    - "í¬ë§íšŒë¡œ ê¸ˆì§€"
    - "ëœ¬êµ¬ë¦„ ê³„íš ê¸ˆì§€"

delegation:
  - agent: "excavator"
    task: "CEO ì˜ë„ ë°œêµ´"
```

**Pragmatist + Skeptic ìŠ¤íƒ ìŠ¤** - êµ¬ì²´í™” í•„ìš”""",

        'coder': f"**CODER** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'excavator': f"**EXCAVATOR** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'strategist': f"**STRATEGIST** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'qa': f"**QA** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'analyst': f"**ANALYST** Mock ì‘ë‹µ - ìš”ì²­: {message}",
        'researcher': f"**RESEARCHER** Mock ì‘ë‹µ - ìš”ì²­: {message}",
    }

    return responses.get(agent_role, f"**{agent_role.upper()}** Mock ì‘ë‹µ - ìš”ì²­: {message}")
